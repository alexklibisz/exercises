{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "- Calculate the probability of a hypothesis given prior knowledge.\n",
    "\n",
    "$P(h|d) = \\frac{P(d|h) \\times P(h)}{P(d)}$\n",
    "\n",
    "- $P(h|d) = $ *probability of hypothesis $h$ given data $d$*, **posterior probability**.\n",
    "- $P(d|h) = $ *probability of data $d$ given hypothesis $h$*.\n",
    "- $P(h) = $ * probability of hypothesis $h$ being true regardless of the data*, **prior probability**.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- $D$ indicates an item is defective.\n",
    "- $A, B, C$ indicate an item was produced at factories A, B, C.\n",
    "- $P(A) = 0.35, P(B) = 0.35, P(C) = 0.3$\n",
    "- $P(D | A) = 0.015, P(D | B) = 0.01, P(D | C) = 0.02$\n",
    "- Find $P(C|D)$ - *probability a defective item was produced at factory C*\n",
    "\n",
    "$P(D|C) = \\frac{P(C|D) \\times P(D)}{P(C)}$\n",
    "\n",
    "$0.02 =  \\frac{P(C|D) \\times P(D)}{0.3}$\n",
    "\n",
    "$P(C|D) = \\frac{0.02 \\times 0.3}{P(D)}$\n",
    "\n",
    "$P(D) = P(D | A) \\times P(A) + P(D | B) \\times P(B) + P(D | C) \\times P(C) = 0.01475$\n",
    "\n",
    "$P(C|D) = \\frac{0.02 \\times 0.3}{0.01475} = 0.406$\n",
    "\n",
    "### Applied to Classification\n",
    "\n",
    "- Features are assumed to be independent so that you don't have to compute each permutation of features given a hypothesis, which would be intractable. This assumption is what makes it *naive*, because typically features will interact in some way.\n",
    "- For categorical features, the model is defined by class probabilities and conditional probabilities. The class probabilities are just the probability of each class in the training dataset. The conditaional probabilities are the probabiliteis of each feature given the corresponding class value.\n",
    "- Predictions are made by compute the maximum probable hypothesis, or the **maximum a-posteriori** hypothesis (MAP) (as opposed to a priori).\n",
    "\n",
    "$MAP(h) = max(P(h|d))$\n",
    "\n",
    "- For real-valued features, Gaussian Naive Bayes is used instead. The mean and standard deviation of each training feature is stored, then predictions are made by determining the probability of that feature's value in using the gaussian probability density function.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- [UCI Iris](https://archive.ics.uci.edu/ml/datasets/Iris)\n",
    "- Four real-valued attributes - so Gaussian Naive Bayes will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Iris accuracy = 0.960000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as ACC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['sep_len', 'sep_wid','pet_len', 'pet_wid', 'class']\n",
    "df = pd.read_csv('./data/iris.data', names=names)\n",
    "\n",
    "for i, name in enumerate(df['class'].unique()):\n",
    "    df['class'].replace(name, i, inplace=True)\n",
    "\n",
    "X = df[['sep_len','sep_wid','pet_len','pet_wid']]\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=44)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print('Scikit-learn Iris Accuracy = %lf' % (ACC(pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Implementation Iris Accuracy = 0.960000\n"
     ]
    }
   ],
   "source": [
    "from math import pi, exp, sqrt\n",
    "\n",
    "\n",
    "class MyGaussianNB:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Categorical implementation.\n",
    "        self._condt_probs = dict()  # Dict stores conditional probabilities.\n",
    "        \n",
    "        # Real-valued implementation.\n",
    "        self._feat_means = dict()   # Dict stores the means.\n",
    "        self._feat_stdvs = dict()   # Dict stores the standard deviations.\n",
    "        \n",
    "        # Common.\n",
    "        self._classes = []          # List of the possible classes.\n",
    "        self._class_probs = dict()  # Dict stores class probabilities.\n",
    "        self._feat_type = dict()    # Dict stores feature types,'cat' or 'con'.\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "    \n",
    "        # Unique classes.\n",
    "        self._classes = list(y.unique())\n",
    "    \n",
    "        # Class probabilities.\n",
    "        for c in self._classes:\n",
    "            self._class_probs[c] = list(y.values).count(c) * 1.0 / len(y.values)\n",
    "                \n",
    "        # Determine feature types and store their corresponding information.\n",
    "        for feat in X.columns:\n",
    "            \n",
    "            iscon = str(X[feat].dtype).startswith('int') or str(X[feat].dtype).startswith('float')\n",
    "            if iscon:\n",
    "                self._feat_type[feat] = 'con'\n",
    "                \n",
    "                # Compute the mean and stdv of this feature given each class.\n",
    "                self._feat_means[feat] = {}\n",
    "                self._feat_stdvs[feat] = {}\n",
    "                for c in self._classes:\n",
    "                    # There's hopefully a less disgusting way to do this.\n",
    "                    matching = [X[feat].values[i] for (i,v) in enumerate(y) if v == c]\n",
    "                    self._feat_means[feat][c] = np.mean(matching)\n",
    "                    self._feat_stdvs[feat][c] = np.std(matching)\n",
    "                \n",
    "            else:\n",
    "                self._feat_type[feat] = 'cat'\n",
    "                # TODO: Compute the conditional probability.\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        feats = list(X.columns)\n",
    "        preds = [self.predict_single(feats, x) for i,x in X.iterrows()]\n",
    "        return preds\n",
    "    \n",
    "    def predict_single(self, feats, x):\n",
    "        \n",
    "        # Gaussian probability density function.\n",
    "        # Takes the value, the mean, and the std deviation.\n",
    "        # Returns the probability of getting that value.\n",
    "        def pdf(x, m, s):\n",
    "            return (1/(sqrt(2*pi)*s))*exp(-((x-m)**2)/(2*s**2))\n",
    "        \n",
    "        # Compute the posterior for each class.\n",
    "        # to find the most likely classification given these features.\n",
    "        # P(c | x) = (P(x | c) x P(c)) / P(x).\n",
    "        # P(x) cancels out because the same features are used.\n",
    "        # So P(c | x) = P(x | c) x P(c).\n",
    "        # P(x | c) = P(x_1 | c) x ... x P(x_n | c).\n",
    "        # P(cls) and P(x_i) are either stored (categorical)\n",
    "        # or easily computed (real-value).\n",
    "        \n",
    "        max_class = None\n",
    "        max_prob = 0\n",
    "        \n",
    "        for c in self._classes:\n",
    "            \n",
    "            prob = self._class_probs[c]\n",
    "            \n",
    "            # Compute P(x | c).\n",
    "            # Categorical features use the stored conditional probabilities.\n",
    "            # Continuous features compute their probability via guassian PDF.\n",
    "            for (feat, x_i) in zip(feats, x):\n",
    "            \n",
    "                if self._feat_type[feat] is 'cat':\n",
    "                    prob *= self._condt_probs[x_i][c]\n",
    "                \n",
    "                elif self._feat_type[feat] is 'con':\n",
    "                    m = self._feat_means[feat][c]\n",
    "                    s = self._feat_stdvs[feat][c]\n",
    "                    prob *= pdf(x_i, m, s)\n",
    "\n",
    "            # Track max probability and its corresponding class.\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                max_class = c\n",
    "            \n",
    "        return max_class\n",
    "    \n",
    "model = MyGaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print('My Implementation Iris Accuracy = %lf' % (ACC(pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "- My custom implementation matches the accuracy (96%) of the scikit-learn implementation for the Iris dataset with continuous features."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
