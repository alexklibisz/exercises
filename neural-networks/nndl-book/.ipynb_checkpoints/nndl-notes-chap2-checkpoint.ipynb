{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning Notes\n",
    "\n",
    "Notes and equations from [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "# Chapter 2\n",
    "\n",
    "- Backpropagation purpose: compute the gradient of the cost function to use in gradient descent.\n",
    "- The partial derivatives $\\frac{\\partial C}{\\partial w} and \\frac{\\partial C}{\\partial b}$ tell us how quickly the cost changes w.r.t changes in the weights and biases.\n",
    "\n",
    "## Warm up: a fast matrix-based approach to computing the output from a neural network.\n",
    "\n",
    "### Notation\n",
    "\n",
    "- $w^l_{jk}$ denotes the weight for the connection from the $k$th neuron in the $(l - 1)$th layer to the $j$th neuron in the $l$th layer. In code, this is expressed as a list of matrices: `w[l][j][k]` is the weight from neuron `k` in layer `l-1` to neuron `j` in layer `l`.\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz16.png)\n",
    "\n",
    "- $b^l_j$ denotes the bias of the $j$th neuron in the $l$th layer. In code, this is expressed as a list of column vectors: `b[l][j]` is the bias from all neurons in layer `l-1` to neuron `j` in layer `l`.\n",
    "- $a^l_j$ denotes the activation of the $j$th neuron in the $l$th layer. In code this is expressed as a list of column vectors: `a[l][j]` is the activation for neuron `j` in layer `l`.\n",
    "\n",
    "- The value of $a^l_j$ is computed: $a^l_j = \\sigma ( \\sum_k w^l_{jk} \\cdot a^{l-1}_k + b^l_j)$. Notice we are using the activation from the layer $l - 1$ to compute the activation at layer $l$. \n",
    "- The same values can also be vectorized by expressing activations, weights, and biases as vectors: $a^l = \\sigma (w^l \\cdot a^l-1 + b^l)$.\n",
    "- $z^l$ is the weighted input to activation function in layer $l$, and it uses the activation of layer $l - 1$: $z^l = w^l \\cdot a^{l-1} + b^l$.\n",
    "\n",
    "\n",
    "## Necessary assumptions about the cost function\n",
    "\n",
    "- Using the quadractic cost function $C = \\frac{1}{2n} \\sum_x (y(x) - a^L(x))^2$, where $y$ is the true output and $a^L(x)$ is the output activation given the current weights and biases.\n",
    "- Cost function can be written as an average of the inidividual sample costs: $C = \\frac{1}{n} \\sum_x C_x$, $C_x = \\frac{1}{2} (y - a^L)^2$\n",
    "- Cost function can be written as a function of the outputs: $C = C(a^L)$, $C = \\frac{1}{2} (y - a^L)^2 = \\frac{1}{2} \\sum_j (y_j - a^L_j)^2$\n",
    "\n",
    "## Hadamard product\n",
    "\n",
    "- Elementwise multiplication of two vectors of the same dimensions.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \n",
    "  \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\n",
    "\\tag{28}\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [8]]\n",
      "[[3 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "# Hadamard example - it seems the np.multiply implicitly does the \n",
    "# hadamard if the dimensions are correct.\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([[1],[2]])\n",
    "b = np.array([[3],[4]])\n",
    "c = np.multiply(a,b)\n",
    "print(c)\n",
    "\n",
    "# Is the hadamard the same as the dot product with one matrix transposed? No.\n",
    "a = np.array([[1],[2]])\n",
    "b = np.array([[3],[4]])\n",
    "c = np.dot(a,b.transpose())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four fundamental equations behind backpropagation\n",
    "\n",
    "### Preliminary: measure of error $\\delta^l_j$ \n",
    "\n",
    "- $\\delta^l_j$ is the error in the $j$th neuron in the $l$th layer. Backprop gives us a procedure to compute this error, which is used to compute the partial derivatives w.r.t. to weight and bias, which are used to compute the gradient.\n",
    "- $\\delta^l_j = \\frac{\\partial C}{\\partial z^l_j}$ is the error at layer $l$ neuron $j$ is the partial derivative of $C$ w.r.t. the weighted input to that neuron ($z^l_j$). \"a measure of the error in the neuron\".\n",
    "\n",
    "### 1. Error in the output layer $\\delta^L$\n",
    "\n",
    "Element-wise form:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\tag{BP1}\\end{eqnarray}$\n",
    "\n",
    "Vectorized form:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L = (a^L-y) \\odot \\sigma'(z^L).\n",
    "\\tag{30}\\end{eqnarray}$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial C}{\\partial a^L_j}$ measures how fast the cost is changing as a function of the $j$th output activation. If $C$ doesn't heavily depend on a particular neuron, then the error $\\delta^L_j$ will be small.\n",
    "- $\\sigma'(z^L_j)$ measures how fast the activation function $\\sigma$ is changing at $z^L_j$. \n",
    "\n",
    "### 2. $\\delta^l$ in terms of the error in the next layer $\\delta^{l+1}$\n",
    "\n",
    "To find the error of any individual layer:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "\\tag{BP2}\\end{eqnarray}$\n",
    "\n",
    "- This equation gets applied to each layer, starting at the penultimate layer and moving backwards through the network.\n",
    "- Use BP1 to get $\\delta^L$, then BP2 to get $\\delta^{L-1}$, $\\delta^{L-2}$, ...\n",
    "\n",
    "### 3. Rate of change of the cost with respect to any bias in the network\n",
    "\n",
    "Element-wise form:\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j.\n",
    "\\tag{BP3}\\end{eqnarray}$\n",
    "\n",
    "Vectorized form:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial b} = \\delta,\n",
    "\\tag{31}\\end{eqnarray}$\n",
    "\n",
    "### 4. Rate of change of the cost with respect to any weight in the network\n",
    "\n",
    "Element-wise form:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\tag{BP4}\\end{eqnarray}$\n",
    "\n",
    "- ROC of $C$ w.r.t. the weight from neuron $k$ in layer $l-1$ to neuron $j$ in layer $l$.\n",
    "- $a^{l-1}_k$ is the activation of neuron $k$ in the previous layer $l-1$.\n",
    "- $\\delta^l_j$ is the error at neuron $j$ in the current layer $l$.\n",
    "- \"Incoming activation times outgoing error\".\n",
    "- Because this is a product, having a very low activation neuron will yield little change even if the error is high. This is a potential pitfall - \"learning slowly\".\n",
    "\n",
    "Simplified form:\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial\n",
    "    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out},\n",
    "\\tag{32}\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proofs of the four fundamental equations\n",
    "\n",
    "TODO\n",
    "\n",
    "## Backpropagation algorithm summarized\n",
    "\n",
    "1. **Input**: The raw features for a single training sample expressed as the activation $a^1$.\n",
    "2. **Feedforward**: For $l = 2,3,...,L$, compute and store the weighted inputs: $z^l = w^l a^{l - 1} + b^l$: and the activations $a^l = \\sigma(z^l)$.\n",
    "3. **Output error**: Compute the output error $\\delta^L = (a^L-y) \\odot \\sigma'(z^L)$.\n",
    "4. **Backpropagate the error**: Going backwards $l = L - 1, L - 2, ..., 2$, compute each $\\delta^{l} = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^{l})$.\n",
    "5. **Compute, store gradients**: At each layer, store the gradients for weight: $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$ and for bias: $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$. These can be stored as lists of matrices of the same size as the bias and weight matrices.\n",
    "6. **Return**: return the stored gradient values $\\nabla C$, which will be used to update the weights and biases for the next iteration.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1) Suppose we modify a single neuron in a feedforward network so that the output is given by $f(\\sum_j w_jx_j + b)$ where $f$ is some function other than the sigmoid. How should we modify backprop for this case?\n",
    "\n",
    "You would need to be able to compute the derivative of that activation function and replace $\\sigma'$ with $f'$ in equations BP1 and BP2.\n",
    "\n",
    "2) Suppose we replace the usual non-linear $\\sigma$ function with $\\sigma z = z$ throughout the network. Rewrite the backpropagation algorith for this case.\n",
    "\n",
    "Not rewriting the entire algorithm, but I believe this would also be a matter of modifying all places that use the derivative of the new $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Backprop\n",
    "\n",
    "This is just a summary of how backpropagation is used to determine the $\\delta w$ and $\\delta b$ that are applied at each iteration of gradient descent.\n",
    "\n",
    "1. Input a set of training examples (`update_by_mini_batch()` function call).\n",
    "2. For each training example $x$:\n",
    "    - Set input activation $a^{x,1}$.\n",
    "    - `back_prop()` function call.\n",
    "    - Feedforward: for each $l = 2,3,...,L$ compute the weighted input to the next layer: $z^{x,l} = w^la^{x,l-1} + b^l$ and the activation at that layer: $a^{x,l} = \\sigma(z^{x,l})$.\n",
    "    - Output error: Compute the error at the output layer: $\\delta^{x,L} = \\nabla_aC_x \\odot \\sigma'(z^{x,L}) = \\delta^L = (a^L-y) \\odot \\sigma'(z^L) = (a^L-y) \\odot \\sigma'(z^L)$.\n",
    "    - Backpropagate the error: For each $l = L -1, L -2, ..., 2$ compute $\\delta^{x,l} = ((w^{l+1})^T\\delta^{x,l+1}) \\odot \\sigma'(z^{x,l})$. \n",
    "\n",
    "3. Gradient descent: use the computed errors to update the weight and bias values:\n",
    "    - For each $l = L, L - 1,...,2$ update weights and biases according to rules:\n",
    "    \n",
    "    $w^l \\rightarrow w^l - \\frac{\\eta}{m}\\sum_x \\delta^{x,l}(a^{x,l-1})^T$\n",
    "    \n",
    "    $b^l \\rightarrow b^l - \\frac{\\eta}{m}\\sum_x \\delta^{x,l}$\n",
    "    \n",
    "\n",
    "## Back-propagation code annotated with the equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(self, x, y):\n",
    "        '''Execute back propogation for a single (x,y) (input, output)\n",
    "        training pair. Return a tuple of the gradients (grad_b, grad_w)\n",
    "        where grad_b and grad_w are layer-by-layer lists of numpy arrays.\n",
    "        \n",
    "        grad_w[l][j][k] is the gradient for the weight from neuron k in layer\n",
    "        l - 1 to neuron j in layer l.\n",
    "        \n",
    "        grad_b[l][j] is the gradient for the bias for neuron j in layer l.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Both gradients initialized as zeros.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Duplicate forward prop, tracking a and z to compute deltas.\n",
    "        a = x   # First activation is the input.\n",
    "        A = [a] # Store the activations.\n",
    "        Z = []  # Store the weighted inputs to activation.\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b # Compute and store the weighted input to the activation.\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)       # Compute and store the sigmoid activation.\n",
    "            A.append(a)\n",
    "            \n",
    "        # Backward pass, compute the error first.\n",
    "        output_delta = quadratic_cost_deriv(A[-1], y) * sigmoid_deriv(Z[-1]) # Eq. BP1.\n",
    "        grad_b[-1] = output_delta # Eq. BP3. Bias gradient at the last layer is the output delta.\n",
    "        grad_w[-1] = np.dot(delta, A[-2].transpose()) # Eq. BP4.\n",
    "        \n",
    "        # Python negative indexing used to loop backward through layers.\n",
    "        for l in range(2, self.num_layers):\n",
    "            sd = sigmoid_deriv(Z[-l]) \n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sd # Eq. BP2.\n",
    "            grad_b[-l] = delta # Eq. BP3.\n",
    "            grad_w[-l] = np.dot(delta, A[-l - 1].transpose()) # Eq. BP4.\n",
    "        \n",
    "        return grad_b, grad_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully matrix-based backpropogation over a mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "SGD Epoch 000: 07238/08000, cost = 0.000000, duration = 4 seconds.\n",
      "0.0\n",
      "SGD Epoch 001: 07446/08000, cost = 0.000000, duration = 4 seconds.\n",
      "0.0\n",
      "SGD Epoch 002: 07483/08000, cost = 0.000000, duration = 5 seconds.\n",
      "0.0\n",
      "SGD Epoch 003: 07560/08000, cost = 0.000000, duration = 5 seconds.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nndl_helpers as helpers\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from __future__ import print_function\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "# Static functions called from within the Network class.\n",
    "def sigmoid(z):\n",
    "    '''Sigmoid activation.'''\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    '''Derivative of the sigmoid activation.\n",
    "    Used in back prop to compute the output error and the layer errors.'''\n",
    "    return np.multiply(sigmoid(z), (np.ones(z.shape) - sigmoid(z)))\n",
    "    #     return sigmoid(z) * (np.ones(z.shape) - sigmoid(z))\n",
    "\n",
    "def quadratic_cost(examples, outputs):\n",
    "    '''Compute the quadratic cost for the outputs relative the \n",
    "    examples (x,y) pairs. Assumes that outputs is a list of column \n",
    "    vectors representing the output activations for the given examples.'''\n",
    "    n = len(examples)\n",
    "    error_sum = 0\n",
    "    for ((x,y), o) in zip(examples, outputs):\n",
    "        error_sum += math.pow(np.sum(y - o), 2)\n",
    "    return (1 / (2*n)) * error_sum\n",
    "\n",
    "def quadratic_cost_deriv(output_activations, y):\n",
    "    '''Derivative of the quadratic cost function. Used in back prop\n",
    "    to compute the output error. Assumes output activations and y are\n",
    "    equal-sized column vectors.'''\n",
    "    return (output_activations - y)\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes=[10,10]):\n",
    "        '''Initialize member variables and randomly initialize \n",
    "        bias and weight values.'''\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # By convention, there are no weights and biases for input layer.\n",
    "        # Biases are a (layer size X 1) column vector.\n",
    "        self.biases = [np.random.randn(sz, 1) for sz in sizes[1:]]\n",
    "        \n",
    "        # Each layer has a (m x n) matrix of weights,\n",
    "        # where m is the size of the layer and n is the size of \n",
    "        # the subsequent layer. \n",
    "        # e.g. layer 1 has size 10, layer 2 has size 11, then layer 1's\n",
    "        # weights are a (10 x 11) matrix.\n",
    "        self.weights = [np.random.randn(m,n) \n",
    "                        for n,m in zip(sizes[:-1],sizes[1:])]\n",
    "        return\n",
    "    \n",
    "    def SGD(self, train, epochs=30, mini_batch_size=10, eta=3.0, test_data=None):\n",
    "        '''Stochastic gradient descent trains the NN in mini-batches.\n",
    "        train should be a list of tuples (x,y) representing the training\n",
    "        samples and their correct outputs.'''\n",
    "        \n",
    "        correct_list = []\n",
    "        cost_list = []\n",
    "        \n",
    "        # SGD broken into epochs.\n",
    "        # Each epoch shuffles the training data into mini-batches\n",
    "        # and iterates over them to incrementally update weights\n",
    "        # and biases using back propogation.\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data and split it into\n",
    "            # non-overlapping mini-batches.\n",
    "            random.shuffle(train)\n",
    "            mini_batches = [\n",
    "                train[k:k+mini_batch_size]\n",
    "                for k in range(0, len(train), mini_batch_size)]\n",
    "            \n",
    "            t0 = time.time()\n",
    "            \n",
    "            # Use each of the mini batches to update biases\n",
    "            # and weights using back propogation.\n",
    "            for mb in mini_batches:\n",
    "                self.update_by_mini_batch(mb, eta)\n",
    "            \n",
    "            t1 = time.time()\n",
    "                \n",
    "            # Evaluate on test data if given, print results.\n",
    "            if test_data:\n",
    "                correct, cost = self.evaluate(test_data)\n",
    "                correct_list.append(correct)\n",
    "                cost_list.append(cost)\n",
    "                print(\"SGD Epoch %03d: %05d/%05d, cost = %lf, duration = %d seconds.\" %\n",
    "                     (epoch, correct, len(test_data), cost, t1 - t0))\n",
    "            else:\n",
    "                print(\"SGD Epoch %d complete, duration = %d seconds.\" % (epoch, t1 - t0))\n",
    "        \n",
    "        return correct_list, cost_list\n",
    "    \n",
    "    def update_by_mini_batch(self, mini_batch, eta):\n",
    "        '''Use back propogation algorithm to compute the gradients\n",
    "        for each (x,y) pair in the mini_batch. Use those gradients\n",
    "        to update self.weights and self.biases at each iteration.'''\n",
    "        \n",
    "        # Gradient sums are the sums of the weight and bias gradients for each neuron\n",
    "        # over all of the samples in the mini batch.\n",
    "        grad_b_mean, grad_w_mean = self.back_prop_matrix(mini_batch)\n",
    "    \n",
    "        self.biases = [b - (eta * gbmean) \n",
    "                       for b, gbmean in zip(self.biases, grad_b_mean)]\n",
    "        self.weights = [w - (eta * gwmean) \n",
    "                        for w, gwmean in zip(self.weights, grad_w_mean)]\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def back_prop_matrix(self, batch):\n",
    "        '''Matrix-based backpropagation algorithm based on last problem in\n",
    "        NNDL chapter 2.'''\n",
    "        \n",
    "        # X is an (n x b) matrix, where b is the number of samples in the batch.\n",
    "        # \"We can begin with a matrix X whose columns are the vectors in the mini-batch.\"\n",
    "        X = np.matrix([x[:,0] for (x,y) in batch]).transpose()\n",
    "        \n",
    "        # Y is a (10 x b) row vector, where each column contains the 10 binary\n",
    "        # classifications of the corresponding column in X.\n",
    "        Y = np.array([y[:,0] for (x,y) in batch]).transpose()\n",
    "        \n",
    "        # Forward propagation.\n",
    "        # \"We forward-propogate by multiplying by the weight matrices,\n",
    "        # adding a suitable matrix for the bias term, and applying the \n",
    "        # sigmoid function everywhere.\"\n",
    "        \n",
    "        A = X       # A is the matrix of activations, starts as all inputs.\n",
    "        A_seq = [A] # Track the sequence of activation matrices.\n",
    "        Z_seq = []  # Track the sequence of weighted input matrices.\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            Z = np.dot(w, A) + b # Apply weight and bias to entire batch in parallel.\n",
    "            A = sigmoid(Z)       # Compute sigmoid of entire batch in parallel.\n",
    "            Z_seq.append(Z)      # Store weighted inputs.\n",
    "            A_seq.append(A)      # Store activations.\n",
    "        \n",
    "        # Backward propagation.\n",
    "        # \"We backpropagate along similar lines.\n",
    "\n",
    "        # Eq. BP1: compute the output error.\n",
    "        # This will be a (10 x b) matrix. Each row corresponds to a digit,\n",
    "        # each column corresponds to a sample from the batch.\n",
    "        D = np.multiply((A_seq[-1] - Y), sigmoid_deriv(Z_seq[-1]))\n",
    "        \n",
    "        # Eq. BP3: bias gradient at the last layer is the output delta.\n",
    "        grad_b = [D]\n",
    "        \n",
    "        # Eq. BP4: weight gradient at the last layer.\n",
    "        grad_w = [np.dot(D, A_seq[-2].transpose())]\n",
    "        \n",
    "        # Back propagate through the layers.\n",
    "        for l in range(2, self.num_layers):\n",
    "            # Eq. BP2: compute error at a layer.\n",
    "            D = np.multiply(np.dot(self.weights[-l + 1].transpose(), D), sigmoid_deriv(Z_seq[-l]))\n",
    "            # Eq. BP3.\n",
    "            grad_b = [D] + grad_b\n",
    "            # Eq. BP4.\n",
    "            grad_w = [np.dot(D, A_seq[-l - 1].transpose())] + grad_w\n",
    "        \n",
    "        # Compute the mean gradients for each neuron at each layer\n",
    "        # to make modifying the biases and weights simple. Each bias\n",
    "        # matrix has a column for each sample in the batch, so it needs to\n",
    "        # be summed row-wise using np.sum. This is not the case for the\n",
    "        # weight matrices, and I still don't fully understand why, but\n",
    "        # it ends up working out.\n",
    "        grad_b_mean = [(np.sum(b, axis=1) / len(batch)) for b in grad_b]\n",
    "        grad_w_mean = [(w / len(batch)) for w in grad_w]\n",
    "        \n",
    "        return grad_b_mean, grad_w_mean\n",
    "    \n",
    "    def back_prop(self, x, y):\n",
    "        '''Execute back propogation for a single (x,y) (input, output)\n",
    "        training pair. Return a tuple of the gradients (grad_b, grad_w)\n",
    "        where grad_b and grad_w are layer-by-layer lists of numpy arrays.\n",
    "        \n",
    "        grad_w[l][j][k] is the gradient for the weight from neuron k in layer\n",
    "        l - 1 to neuron j in layer l.\n",
    "        \n",
    "        grad_b[l][j] is the gradient for the bias for neuron j in layer l.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Both gradients initialized as zeros.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Duplicate forward prop, tracking a and z to compute deltas.\n",
    "        a = x   # First activation is the input.\n",
    "        A = [a] # Store the activations.\n",
    "        Z = []  # Store the weighted inputs to activation.\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b # Compute and store the weighted input to the activation.\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)       # Compute and store the sigmoid activation.\n",
    "            A.append(a)\n",
    "            \n",
    "        # Backward pass, compute the error first.\n",
    "        output_delta = quadratic_cost_deriv(A[-1], y) * sigmoid_deriv(Z[-1]) # Eq. BP1.\n",
    "        grad_b[-1] = output_delta # Eq. BP3. Bias gradient at the last layer is the output delta.\n",
    "        grad_w[-1] = np.dot(delta, A[-2].transpose()) # Eq. BP4.\n",
    "        \n",
    "        # Python negative indexing used to loop backward through layers.\n",
    "        for l in range(2, self.num_layers):\n",
    "            sd = sigmoid_deriv(Z[-l]) \n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sd # Eq. BP2.\n",
    "            grad_b[-l] = delta # Eq. BP3.\n",
    "            grad_w[-l] = np.dot(delta, A[-l - 1].transpose()) # Eq. BP4.\n",
    "        \n",
    "        return grad_b, grad_w\n",
    "        \n",
    "    def evaluate(self, validate):\n",
    "        '''Evaluate using the member variable biases, weights and the given\n",
    "        validation data. The validation data should be a list of tuples (x,y).\n",
    "        Return the number of correct classifications and the cost.'''\n",
    "        \n",
    "        # Compute outputs for each (x,y) pair using forward prop.\n",
    "        outputs = [self.forward_prop(x) for (x,y) in validate]\n",
    "        \n",
    "        num_matches = sum([int(np.argmax(o) == np.argmax(y))\n",
    "                          for (o, (x,y)) in zip(outputs, validate)])\n",
    "        \n",
    "        cost = quadratic_cost(validate, outputs)\n",
    "        \n",
    "        return num_matches, cost\n",
    "    \n",
    "    def forward_prop(self, inputs):\n",
    "        '''Computes the network output given the input a.\n",
    "        Uses biases and weights to multiply the activations as\n",
    "        they pass through the network.'''\n",
    "        \n",
    "        # Inputs are treated as the first activation.\n",
    "        a = inputs\n",
    "        \n",
    "        # The activation at layer i is the sigmoid function\n",
    "        # evaluated on the current weights and biases and the\n",
    "        # activation from layer i - 1.\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        \n",
    "        # The output is the last activation.\n",
    "        outputs = a\n",
    "        return outputs\n",
    "\n",
    "# Retrieve 60000 training, 8000 testing, 2000 validation samples.\n",
    "train, test, validate = helpers.get_mnist_data()\n",
    "\n",
    "t0 = time.time()\n",
    "net = Network([784,60,60,10])\n",
    "num_matches_list, cost_list = net.SGD(train, epochs=15, mini_batch_size=10, eta=3.0, test_data=test)\n",
    "t1 = time.time()\n",
    "print('Training time in seconds = %d' % (t1 - t0))\n",
    "\n",
    "num_matches, cost = net.evaluate(validate)\n",
    "print('Validation: %d / %d, %.4lf, cost = %.4lf' %\n",
    "    (num_matches, len(validate), num_matches / len(validate), cost))\n",
    "\n",
    "plt.title('Cost over epochs.')\n",
    "plt.plot(cost_list)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Number of matches over epochs.')\n",
    "plt.plot(num_matches_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Gradient descent boils down to those four equations (BP1, BP2, BP3, BP4).\n",
    "- Good summary: \"The backpropagation algorithm is a clever way of keeping track of small perturbations to the weights (and biases) as they propagate through the network, reach the output, and then affect the cost.\"\n",
    "- The matrix-based back-propagation implementation ends up being ~2x faster than the original implementation. On an i7860 @ 2.80 GHz, matrix-based takes ~15 seconds per epoch while regular takes ~30 seconds per epoch.\n",
    "- To debug the matrix-based implementation, it is helpful to print out the sizes of the weight and bias matrices and compare them to the other implementation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
