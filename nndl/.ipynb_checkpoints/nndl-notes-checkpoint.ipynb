{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning Notes\n",
    "\n",
    "Notes and equations from [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "# Chapter 1\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "The **simple perceptron** computes its output as a weighted sum of the inputs\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\tag{1}\\end{eqnarray}\n",
    "$\n",
    "\n",
    "The notation can be adjusted to use a dot product and a **bias** term instead of the threshold. The bias controls how easy or difficult it is to get the perceptron to output a 1.\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\tag{2}\\end{eqnarray}\n",
    "$\n",
    "\n",
    "The perceptron can implement a NAND gate:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz2.png)\n",
    "\n",
    "$00 = -2 * 0 + -2 * 0 + 3 = 3 \\rightarrow 1$\n",
    "\n",
    "$01 = -2 * 0 + -2 * 1 + 3  = 1 \\rightarrow 1$\n",
    "\n",
    "$10 = -2 * 1 + -2 * 0 + 3  = 1 \\rightarrow 1$\n",
    "\n",
    "$11 = -2 * 1 + -2 * 1 + 3  = -1 \\rightarrow 0$\n",
    "\n",
    "The perceptron can implement a two bit adder. All weights are -2 except for the one marked -4 and all biases are 3:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz6.png)\n",
    "\n",
    "## Sigmoid Neurons\n",
    "\n",
    "Perceptrons are very sensitive to changes in their input (small input change can cause large output change), which makes it difficult to apply learning algorithms to them. Sigmoid neurons help to alleviate this because the sigmoid function has smaller changes in its output - effectively a smoothed out perceptron.\n",
    "\n",
    "The **sigmoid function**:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\tag{3}\\end{eqnarray}$\n",
    "\n",
    "The output of a sigmoid neuron with weights $w_i$, inputs $x_i$, and bias $b$:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
    "\\tag{4}\\end{eqnarray}$\n",
    "\n",
    "Behavior: when $w \\cdot x + b$ is large, the sigmoid output approaches 1; when it's small, the output approaches 0. The outputs are continuous values between 0 and 1, so you need some heuristic or convention to interpret meaning from them. For example, you might say that any value greater than or equal to 0.5 indicates a \"yes\", and any value less than 0.5 indicates a \"no\".\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1) Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a constant, $c > 0$. Show that the behavior of the network doesn't change.\n",
    "\n",
    "The perceptron's output is determined by the weighted sum of the inputs and bias compared to 0, $w \\cdot x + b > 0$. Multiplying both $w$ and $b$ by a constant $c$ gives us the inequality $cw \\cdot x + cb = c(w \\cdot x + b) > 0$. We can show they are equal:\n",
    "\n",
    "$\n",
    "cw \\cdot x + cb > 0\n",
    "\\\\ c(w \\cdot x + b) > 0\n",
    "\\\\ w \\cdot x + b > \\frac{0}{c}\n",
    "\\\\ w \\cdot x + b > 0\n",
    "$\n",
    "\n",
    "The same can be easily shown for the $\\leq 0$ inequality.\n",
    "\n",
    "2) Given a network of perceptrons and a fixed input to the network. The weights and biases are such that $w \\cdot x + b \\neq 0$ for input $x$ to any perceptron in the network. Replace all perceptrons by sigmoid neurons. Multiply the weights and biases by a positive constant $c > 0$. Show that in the limit as $c \\rightarrow \\infty$, the behavior of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w \\cdot x + b = 0$ for one of the perceptrons?\n",
    "\n",
    "It seems I should show that as $c \\rightarrow \\infty$, $\\frac{1}{1 + e ^ {-c(w \\cdot x + b)}}$ \"behaves\" the same as $c(w \\cdot x + b)$. I'm not certain how to approach the topic of \"behavior\" at a network level - should I show that they increase at the same rate? A single case could probably be worked out using the two bit adder network above.\n",
    "\n",
    "## The Architecture of Neural Networks\n",
    "\n",
    "Terminology\n",
    "\n",
    "- Multilayer perceptrons are actually made up of sigmoid neurons (there can be other activation functions used as well).\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "\n",
    "- Images will be 28 by 28, meaning the input layer has 27 x 27 = 784 neurons.\n",
    "- There are ten output neurons - each will have an activation value between 0 and 1. The digit gets classified by taking the neuron with the highest activation value.\n",
    "- You could also do a bit-wise output representation. This would have four neurons, each of which can represent 0 or 1. This would work because all digits from 0 to 9 can be represented with four binary bits (e.g. 0 = 0000, 4 = 0100, 9 = 1001). It turns out the 10-neuron representation performs better.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Suppose you had the 10-neuron output layer but wanted to downsample it to a 4-neuron (bit-wise) output layer. Find a set of weights that makes this possible. Assume that correct outputs in the 3rd layer (old output layer) have activation gte 0.99 and incorrect have activation < 0.01.\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz13.png)\n",
    "\n",
    "We can consider this to be a linear system of equations, $Aw = B$, where $A$ is the activation values from the old output layer, $w$ is the matrix of weights we are looking for, and $B$ is the activation of the bit-wise encoded new output layer.\n",
    "\n",
    "This allows us to simply populate the matrices $A$ and $B$ and solve for the weight matrix $w$.\n",
    "\n",
    "$A$ and $B$ will look like this:\n",
    "\n",
    "$A = \\{\\{0.99, 0.001, ...\\}, \\{0.001, 0.99, 0.001, ...\\}, \\{0.001, 0.001, 0.99, 0.001, ...\\}, ... \\}$\n",
    "$B = \\{\\{0, 0, 0, 0\\}, \\{0, 0, 0, 1\\}, ...\\}$\n",
    "\n",
    "See code below for the computed solution to this sytem of equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aw = b, A = :\n",
      " [[ 0.99   0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001]\n",
      " [ 0.001  0.99   0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001]\n",
      " [ 0.001  0.001  0.99   0.001  0.001  0.001  0.001  0.001  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.99   0.001  0.001  0.001  0.001  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.99   0.001  0.001  0.001  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001  0.99   0.001  0.001  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001  0.001  0.99   0.001  0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.99   0.001  0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.99   0.001]\n",
      " [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.99 ]]\n",
      "Aw = b, w = :\n",
      " [[-0.002024 -0.004049 -0.004049 -0.005061]\n",
      " [-0.002024 -0.004049 -0.004049  1.006062]\n",
      " [-0.002024 -0.004049  1.007074 -0.005061]\n",
      " [-0.002024 -0.004049  1.007074  1.006062]\n",
      " [-0.002024  1.007074 -0.004049 -0.005061]\n",
      " [-0.002024  1.007074 -0.004049  1.006062]\n",
      " [-0.002024  1.007074  1.007074 -0.005061]\n",
      " [-0.002024  1.007074  1.007074  1.006062]\n",
      " [ 1.009098 -0.004049 -0.004049 -0.005061]\n",
      " [ 1.009098 -0.004049 -0.004049  1.006062]]\n",
      "Aw = b, B = :\n",
      " [[-0.  0.  0.  0.]\n",
      " [-0. -0. -0.  1.]\n",
      " [-0. -0.  1. -0.]\n",
      " [-0. -0.  1.  1.]\n",
      " [-0.  1. -0. -0.]\n",
      " [-0.  1.  0.  1.]\n",
      " [-0.  1.  1. -0.]\n",
      " [-0.  1.  1.  1.]\n",
      " [ 1. -0. -0. -0.]\n",
      " [ 1. -0. -0.  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzh/anaconda3/lib/python3.5/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((10, 4), 0) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.full((10,10), 0.001)\n",
    "B = np.full((10,4), 0)\n",
    "\n",
    "# Fill in each digit with a 0.99 in A.\n",
    "# Fill in the binary representation of each digit in B.\n",
    "for i in range(0,10): \n",
    "    A[i][i] = 0.99\n",
    "    B[i] = [int(b) for b in list('{0:04b}'.format(i))]\n",
    "\n",
    "# Solve for w in Aw = B. This is the weight matrix.\n",
    "w = np.linalg.solve(A, B)\n",
    "\n",
    "# Print results.\n",
    "print('Aw = b, A = :\\n', A)\n",
    "np.set_printoptions(precision=6)\n",
    "print('Aw = b, w = :\\n', w)\n",
    "np.set_printoptions(precision=1)\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Aw = b, B = :\\n', np.dot(A,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Gradient Descent\n",
    "\n",
    "### Notation\n",
    "\n",
    "- $X$ denotes the matrix of all trainign input vectors.\n",
    "- $x$ denotes a single 28 x 28 training input as a $784 \\times 1$ vector.\n",
    "- $w$ denotes the collection of all weights in the network.\n",
    "- $b$ denotes the collection of all biases.\n",
    "- $n$ is the total number of inputs.\n",
    "- $a$ is the vector of outputs from the network given a single input $x$.\n",
    "- $y(x)$ is the correct vector of outputs for a single input $x$.\n",
    "- $\\eta$ is the learning rate\n",
    "\n",
    "### Quadratic cost function (aka mean squared error)\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\sum_{x \\in X} (y(x) - a)^2$\n",
    "\n",
    "- Minimize this measure by adjusting $w$ and $b$ using gradient descent.\n",
    "- We use this proxy measure of accuracy because classification accuracy alone is not a smooth function, so it's difficult to minimize.\n",
    "\n",
    "### Gradient Descent Intuition\n",
    "\n",
    "Repeatedly compute a gradient $\\nabla C$, which is used to update the parameters to the cost function. This will move you towards the minimum of the cost function.\n",
    "\n",
    "### Gradient Descent Explained Mathematatically for a Two-variable Cost Function\n",
    "\n",
    "- Goal: Minimize the cost function $C(v_1, v_2)$.\n",
    "\n",
    "- Strategy: Iteratively adjust $v_1$ and $v_2$ in order to reach a global minimum for $C$.\n",
    "\n",
    "When $C(v_1, v_2)$ is changed by $(\\Delta v_1, \\Delta v_2)$, this results in a $\\Delta C$ expressed:\n",
    "\n",
    "$\\Delta C = \\frac{\\partial C}{\\partial v_1} \\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2$\n",
    "\n",
    "It's important to understand that $\\frac{\\partial C}{\\partial v_1} \\Delta v_1$ really means \"the rate at which $C$ is changing with respect to $v_1$\" (... $\\frac{\\partial C}{\\partial v_1}$ ...) times the amount by which $v_1$ is changing ( ...$\\Delta v_1$... ). This is analagous to saying \"distance = velocity x duration\". $\\Delta C$ is just the sum of these distances for both $v_1$ and $v_2$.\n",
    "\n",
    "- Checkpoint: How does $\\Delta C$ affect $C$? \n",
    "\n",
    "$C \\rightarrow C' = C + \\Delta C$.\n",
    "\n",
    "So if we can find values for $(\\Delta v_1, \\Delta v_2)$ such that $\\Delta C$ is negative, then we can iteratively minimize $C$.\n",
    "\n",
    "- Checkpoint: How do we find values for $(\\Delta v_1, \\Delta v_2)$ to make $\\Delta C$ negative?\n",
    "\n",
    "First we define the \"gradient\" of $C$ as:\n",
    "\n",
    "$\\nabla C = (\\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2})^T$\n",
    "\n",
    "and the change in $v$ is defined as:\n",
    "\n",
    "$\\Delta v = (\\Delta v_1, \\Delta v_2)^T$\n",
    "\n",
    "Remembering that $\\frac{\\partial C}{\\partial v_1} \\Delta v_1$ is two separate entities $\\frac{\\partial C}{\\partial v_1}$ and $\\Delta v_1$, we can rewrite $\\Delta C$ in a more convenient way:\n",
    "\n",
    "$\n",
    "\\Delta C = ((\\frac{\\partial C}{\\partial v_1})(\\Delta v_1) + (\\frac{\\partial C}{\\partial v_2})(\\Delta v_2))\n",
    "\\\\ \\\\ \\Delta C = (\\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2})^T \\cdot (\\Delta v_1, \\Delta v_2)^T\n",
    "\\\\ \\\\ \\Delta C = \\nabla C \\cdot \\Delta v\n",
    "$\n",
    "\n",
    "So we've re-written $\\Delta C$ as the dot product of $\\nabla C$ and $\\Delta v$.\n",
    "\n",
    "- Checkpoint: How do we choose $\\Delta v$ to make $\\Delta C$ negative?\n",
    "\n",
    "We already know what $\\nabla C$ and $\\eta$ are. So for convenience, lets choose:\n",
    "\n",
    "$\\Delta v = -\\eta \\nabla C$.\n",
    "\n",
    "This lets us re-write $\\Delta C$ as:\n",
    "\n",
    "$\\Delta C = \\nabla C \\cdot -\\eta \\Delta v = -\\eta (\\nabla C)^2$\n",
    "\n",
    "Because $-\\eta (\\nabla C)^2 < 0$, $\\Delta C$ will always be negative, which is what we wanted in the first place.\n",
    "\n",
    "- Checkpoint: How do we update $v$ to \"move\" towards minimizing $C$?\n",
    "\n",
    "We've determined that choosing $\\Delta v = -\\eta \\nabla C$ will eventually minimize $C$, so then we actually update $v$ as follows:\n",
    "\n",
    "$v \\rightarrow v' = v + \\Delta v = v - \\eta \\nabla C$\n",
    "\n",
    "Again, this guarantees that $\\Delta C$ will always decrease.\n",
    "\n",
    "### Gradient Descent for Multi-variable Cost Functions\n",
    "\n",
    "Now you have $m$ variables $(v_1, ..., v_m)$.\n",
    "\n",
    "Everything else is the same, just adjusted for the $m$ variables:\n",
    "\n",
    "- Cost function: $C(v_1, ..., v_m)$.\n",
    "- Gradient of $C$: $\\nabla C = ( \\frac{\\partial C}{\\partial v_1}, ..., \\frac{\\partial C}{\\partial v_m})$.\n",
    "- Change in $v$: $\\Delta v = (\\Delta v_1, ..., \\Delta v_2) = -\\eta \\nabla C$.\n",
    "- Change in $C$: $\\Delta C = \\nabla C \\cdot \\Delta v$.\n",
    "- Update rule: $v \\rightarrow v' = v - \\eta \\nabla C$.\n",
    "\n",
    "### Application of Gradient Descent\n",
    "\n",
    "- Finding the weights $w_k$ and biases $b_i$ to minimize the quadratic cost function.\n",
    "- Update rules:\n",
    "    - weights: $w_k \\rightarrow w'_k = w_k - \\eta \\frac{\\partial C}{\\partial w_k}$\n",
    "    - biases: $b_l \\rightarrow b'_l - \\eta \\frac{\\partial C}{\\partial b_l}$\n",
    "\n",
    "### Application of Stochastic Gradient Descent\n",
    "\n",
    "- Gradient Descent can be expensive because it requires computing and storing a $\\nabla C_x$ for each training sample $x$ to find $\\nabla C = \\frac{1}{n} \\sum_x \\nabla C_x$\n",
    "- One solution to speed things up is **stochastic gradient descent**, which approximates $\\nabla C$ by computing $\\nabla C_x$ for a random subset of $m$ training samples, called a **mini-batch**:\n",
    "- Approximated gradient:\n",
    "    $\\nabla C \\approx \\frac{1}{m} \\sum_{j=1}^m \\nabla C_{X_j}$\n",
    "- Thus, the update functions also use an approximation:\n",
    "    - weights: $w_k \\rightarrow w'_k = w_k - \\nabla C = w_k - \\frac{\\eta}{m} \\sum_j \\frac{\\partial C_{X_j}}{\\partial w_k}$ \n",
    "    - biases: $b_l \\rightarrow b'_l = b_l - \\frac{\\eta}{m} \\sum_j \\frac{\\partial C_{X_j}}{\\partial b_l}$\n",
    "- Sometimes the averaging is omited and the entire sum is used as the update value (e.g. when the amount of training samples is unknown at runtime).\n",
    "\n",
    "### Exercise\n",
    "\n",
    "An extreme version of gradient descent uses a mini-batch size of 1. Name 1 advantage and disadvantage:\n",
    "\n",
    "- Advantage: computationally less expensive, arguably simpler to just compute one gradient at a time.\n",
    "- Disadvantage: sensitive to outliers - if a very irregular sample is randomly selected, it could cause a jump in $C$ that doesn't progress towards minimizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our Network to Classify Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Epoch 000: 07296/08000, cost = 0.0191\n",
      "SGD Epoch 001: 07389/08000, cost = 0.0154\n",
      "SGD Epoch 002: 07445/08000, cost = 0.0160\n",
      "SGD Epoch 003: 07527/08000, cost = 0.0129\n",
      "SGD Epoch 004: 07553/08000, cost = 0.0129\n",
      "SGD Epoch 005: 07568/08000, cost = 0.0124\n",
      "SGD Epoch 006: 07556/08000, cost = 0.0125\n",
      "SGD Epoch 007: 07570/08000, cost = 0.0116\n",
      "SGD Epoch 008: 07580/08000, cost = 0.0111\n",
      "SGD Epoch 009: 07580/08000, cost = 0.0116\n",
      "Validation: 1929 / 2000, 0.9645, cost = 0.0077\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFyCAYAAACpypMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb1478e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFyCAYAAACgITN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3Xl4VOX5//H3DQIKKqAWcUOLC2IVK8GFr0pVcKulam2r\nAVfqhooUd3+urfuGKGhFQaEuUcG6VFFUUMEVJLZWCmhZFBdkDyiCkNy/P54zchgmkJkknJnk87qu\nucI855kz95kJmXue1dwdERERkVw0SDoAERERKVxKJERERCRnSiREREQkZ0okREREJGdKJERERCRn\nSiREREQkZ0okREREJGdKJERERCRnSiREREQkZ0okpM4zs1+ZWYWZ/S7pWKrCzFqZ2Ugzm29m5WZ2\nYdIxVYeZ7Ri9/hclHYvUnOg9vTfpOCR5SiSkRpjZadEflmVmtk2G42+a2cdJxBYppLXgBwCHAzcB\npwCvJBsOmFmxmfVNOg4RyT9KJKSmNQGuyFCe9Ae5Jfz82TgUeM7d73b3J9z906QDAnoASiREZC1K\nJKSm/Qs4y8xaJx3IhmZmTWvoVK2Asho6l+QpM2toZo2SjkOkupRISE1y4GZgIzK3Svwk1m9+aoZj\nFWZ2bez+9VHZrmb2mJktNrO5ZvbX6PgOZvacmZWZ2TeV9MU70NDMbo7qfGdmz5vZ9hmef38zeyV6\nnu+jbpn/S6uTiqm9mT1hZguB8eu55p+b2QgzWxCd9z0z+3Xs+GlmVhHdvSA6f3kVXsOLzOw8M5se\nnXe0mW0X1bnGzGZHXU7PmVmLtHP81sxeNLOvzGy5mf3PzK42swaxOm8AxwCp56swsxmx402i12Oa\nmf1gZl+b2TNm9vMMMZ8VPcdyM5tgZp0y1GkXjRFZEJ1vopl1T6uzkZldZ2afRnXmm9l4M+u6rveg\niu9DKzNbaWbXZHjsbtH1nxcra25mA8zsi+i6PjOzy8zMYnXi71VfM/sfsBxov55YTzazD6P3b4GZ\nlaT/zka/nx+bWUczeyeqO8PMzslwvp+Z2VAzmxO9bv+yzP8HLYrz46jeXDN72cw6Zqh7rJn9J7r2\nT8zsyLTjm0avz8yozrdm9qqZ/XJd1y6FY6OkA5A6Zybwd0KrxK3uPqcGzpnqFnkK+C9wOeGD7SoL\nH+DnAGOAy4CewB1mNsHd346dw4CrgQrgVsK3/n7Aa2b2S3dfAWBmhwGjgA+B66P6ZwBjzewgd/8w\nLaYRwKfAlayj+8TMWgHvARsD9wALgdOAF8zsBHd/HngLOBl4DHiV8DpWxclAI+BeYIvo9RlhZmOB\nX0XXuwtwIXAncGbssacDS4G7gO+Aw4C/AptF5wG4EWgObAf8ObrO76LragC8ROiOKSGM79iMMMZj\nT8LvQ0pPYFPgAcLrdznwjJm1dffy6Hy/AN4GvgRuAb4H/gg8Z2a/i14ngL8QktUHgYnA5kAnoCPh\ndyGjqrwP7j7XzN6KnveGtFOcBKwivO+Y2SbAOGCb6LpmA/8Xxd4aSE9qexG6/wYDK6LnryzWqwjv\nxZPAQ8DPCO/hW2a2j7sviao64X1/CXgaeCKK/W9mtsLdh0Xn25jwO9YWGAjMAv4ADDOz5u4+MPb0\nD0evy0vRc28EHAwcAJTG6h0M/A64n/B7dCEw0szauPuiqM7gqM5AYAqwJXAQIYn6V2XXLwXE3XXT\nrdo3wh+dcsIf8p8DPwJ3x46/AXwcu78j4UP61AznqgCujd2/Liq7P1bWAPiC8Ef9klh5c8KHz8Ox\nsl9Fj/8CaBor/31UfkGsbBrwUlo8TYDpwCsZYnq0iq/P3dHr0zlW1iw67/QM139vFc6Zeg3nAJvG\nym+KykuBBrHyx4EfgEbxa8tw3r8RPhTi9f4JzMhQ94zouS6sQpxzgc1j5d2j1+TXsbLXgY+AjdLO\n8TYwNXb/I+CFHH5Pq/Q+AGdF9fZIe/wnwGux+1cDS4C2afVujv4PbJf2GiwCtqhCnG2AlcDlaeV7\nROe9Iu3/VjnQN1bWKHr/vwEaRmV9o3onxeo1BN4hdKU1i8oOjWLtv54YK6Lfp51iZXtF5efFyhZV\n5fdZt8K9qWtDapy7zwQeBc42s61r6rTA0NhzVBBaDYzw7SlVXkZIBtpmOMdwd18WqzuS8If21wBm\ntg+wK1BiZlumboRv2GOALhliGlzF+I8GJrj7e7Hn/57wjXonM9ujiufJ5Gl3/y52/4Po56PR6xQv\nb0xoWUjFsCL176gJekvCh3ZTYPcqPPfvgHnAoCrUfdJXf4uG0BVkRO+VmbUkfIiNAJqnvQevArva\n6hlBi4FfmNkuVXjeuKq+D/8gfOiemKoXtZbsQWghSPl9dB1lafGOIXyLT/+dGenulbZCxJxAeG1G\npJ13LvAZ4XWKWxVdQ+qaVhJ+N1sBRbFrn+PuT8bqlRNasjYlJNyp564gtIasz2vuPit2vv8QJVax\nOouB/S3DbC6pG5RISG25kfCtaJ1jJbL0Rdr9MmB5hj/MZUDLDI//XyVlO0X/Tn0o/Z3w4Zi6zSV0\nBzQ2s+Zpj59J1exISHDSTYkdz9XstPupgZpfVlL+02tjZnuY2bNmtpjwATCPkARCaN1Zn52BaWkJ\nS5XidPfFafHsQvjwvIE1X/95hG4mCB+MANcCLYBPo378281sryrEUKX3wd0XEJKBP8bqnERoJXg2\nVrYrcFSGeF8jJJqtWNOsKsQI4bVoQPj9TP9d3D3Deb929x/Syj4lvJ47xa7tswzPNSWql/odbBud\nb3GGuunSf/cgtEDE//9dRujmmm1mH0RjW9YaPyOFS2MkpFa4+0wze4zQKnFbpiqZHmexQX4ZZBp4\nWNlgxFyme6ae+2Lg35XU+S7tfvof7yRU9hqs87WJkqJxhG+MVwMzCAMAiwjjKmr6i8b63qvU890J\njK6k7v8A3H28me0MHAscAfwJ6Gdm57j7w5U8NltPAg+bWQd3/5gwnmBMWuLagJA03Ebm37n0qbtV\n/X1pQGgVOCr6mS799zAp6/3/5+4jzGwccDzhvboEuNzMjnf3yt5nKSBKJKQ23UgYCHh5hmOpgVgt\n0sqr8818fXbNULYLq5OG6dHPpe4+toaf+3OgXYby9rHjG9ohhG+Ox7r7O6nC6AM6XWXrgEwH9jOz\nhlEzeXWkZoKsrMrrH31jHg4MtzD1djyh5WJdiUQ278NzhO6BE6MZGLsRxp/ETSeMT3ljffFmaTrh\nw3iWu2dqSUu3rZltktYq0Y7wvqVazT4njGFIl7r2WbHnPsLMWlSxVWK93P1bwmDUB8xsK8IYl6uo\nPGGUAqKuDak17j6DMAPhHMII9vixpcB81u5DPp/aW7zqVDPbNHXHzP5AGG0/KiqaRPgjeomZNUt/\ncPQHMFejCB+4+8fO1ww4G5jp7v+txrlzVU74sIpP9WwMnJeh7vdk7up4hjCb4ILqBuPu84A3gXMs\nwzok8dffzLZIe+wyQmtFk/U8TZXfh2i8zWhC98ZJhFkWz695Op4GOpvZERnibW5mDdcTT2X+QWiJ\nuC7TwfTrJ3wpPDd2vBHh/908Vs+yGAW0NrP4uI+GQB/C4NpxUfEzhN+JjM+dDTNrYGabx8vcfT7w\nNbH3KhoD0i6aBSMFRi0SUpMyNe2mlnluRxjxHjcEuMLMHiIMnOxCaDWorVUoFwJvm9kjhMSmL6Hp\neQiAu7uZnUn4gzs5qvcVYXDioYQxBsfm+Ny3AsXAKxb2J1hImHq5I2HA4oYSf23fJbQM/d1W75lw\nMpkTuUnAH83sLsJ0y+/c/UXCeJJTgf7Rh/N4wsC9rsB97v7PLOM7PzrHf6LfixnA1kBnwvuwT1Tv\nv2b2ZhTXQmBfwsDH9e39kO378BQhGT4PGJ02WBTgDuC3wItmNiyKpxnQITrfTqxjimdl3H2GmV0N\n3ByNJ3iO8GHfFjiO0FLSP/aQr4HLzGwnwu/0SVEMZ8Vaih4kJBfDLKzfMYvQXdOZMOPj++i53zSz\nR4ELzWw3whLtDQhTPce6+/1ZXMpmwJdmNpLQ8vcdYWpwJ9acGtuHMO7lEFYnNFIglEhITVrrA8jd\np0d/lE7LcPyvwFaED4A/ED7AjyYMKKtqq0Rl9dLLU4tldSAMAN2M0Ld9vrsvj8X7lpl1Bq4hfKht\nSphe+QFVn6GxdjBhbYLOhL70CwjrGHwM/Mbd0/fS8HVc11qnrqTuel8Xd19oZscQ1pC4gZBUPAqM\nZe0m5/uBvQkfun8mNJO/6O4VZnY0oZm6B+HDcwFRMlDFOOMxTYk+5K4j/M6kZip8xJqzCO4hfIAf\nTvhm+znw/wjjKyqV5fsA8AJhXEMz1pytkTrfD2bWJXruPxCS5iWED/NrWXOF0mzeV9z9NjObRljv\nJLVA22zCB/sLadUXEV6vQYSBwd8SfrfjM5qWm1lqXZFTCWtvTANOd/dH0853OuGD/0/A7dF1fEhI\nPtd3PfHyZcB9hLERx7N6AGlvd3+wksdIgTF3vXciIoXKwsqjW7p7h6RjkfopqzESFpY4rchwGxir\n097C0sOLLSxD/IHFlnS1sJzufRaWtF1qYSncVmnP09LMHrew5PEiMxuSqc9aREREkpXtYMtOhL7l\n1O1wQnPU0/DTaO/xhGWMuxBGCN9AmFKWMoCwvPEJUZ1tCYN74p4gjCTuGtXtQjWalUVERKR2VKtr\nw8wGEJa33S26XwL86O6nVVJ/c8Io4pPc/dmorB1hQZQD3H2CmbUHJgNF7v5RVOdIwprv23vN7N0g\nIlInRF0bW7j73knHIvVTztM/o+lFPYmWLY7mWR8DfGZh58Rvzex9M4uPci8iDPD8aVMdd59GWLGw\nc1R0ALAolUREXie0fOyPiIj8xN0PVRIhSarOrI3jCfPKh0f3WxFGuF9OGMF9GWEE/j/M7BB3H0/o\nDvkxwxSqb1m9zkBrwijtn7h7uYVdHteaW54SrUN/JGFK0/LK6omIiMhaNiZMVx4dLRFfZdVJJHoB\nL8e6GlKtG8+5e2ou98dm9n+EhVLGV+O5quJIwu6GIiIikpuehHGKVZZTImFmbYBuhIVRUuYTdqCb\nklZ9CnBg9O85hI2PNk9rldg6Opaqkz6LoyGwRaxOJrMAHnvsMdq3b7+OaoWjX79+3H333UmHUSPq\n0rWArief1aVrAV1PPqtL1zJlyhROPvlkqPrGcj/JtUWiF6E7IrW0MO6+0swmsvY69ruxev36SYRk\noyvRDnrRYMs2QGpb3/eAFma2T2ycRFfCinyp7ZEzWQ7Qvn17OnbsmONl5ZfmzZvrWvKUrid/1aVr\nAV1PPqtL1xKT9dCArBOJaFDl6cCwDFsH3wE8aWbjgTcIYyR+Q7TPvbsvMbOhhOV0FxGWfL0XeMfd\nJ0R1pprZaOAhM+sNNAYGAiWasSEiIpJfcmmR6AbsADySfsDdnzOzcwnLxd5DWH71d+7+XqxaP8Jm\nQSMJS9u+QliKOK4HYanX1wkb14wk7IsgIiIieSTrRMLdXwMq3dHO3YcBw9ZxfAVhg5Y+66izmLB5\nkIiIiOQxbSOex4qLi5MOocbUpWsBXU8+q0vXArqefFaXrqU66symXWbWEZg0adKkujj4RUREpNaU\nlpZSVFQEYVXp0mweqxYJERERyZkSCREREcmZEgkRERHJmRIJERERyZkSCREREcmZEgkRERHJmRIJ\nERERyZkSCREREcmZEgkRERHJWa7biIuIiNQ7ZWUweTJ88gnMnAkV6XtgF6g51dhbW4mEiIhImh9+\ngClTQsIQv82eHY43aABt2kCjRsnGWVOWL8/9sUokRESk3lq5Ej79dO2EYfp0SG1FtdNOsOee0LNn\n+LnnntCuHWy8caKh16jSUghbbWRPiYSIiNR55eWhK+KTT1Z3TXzyCUybFpIJgG22CUlC9+6rE4Y9\n9oBNN0029nynREJEROoMd/jqq7VbGP7739BdAdCyJey1F3TpAr17h4ThF7+ALbdMNvZCpURCREQK\n0rx5aycMkyeHAZEAzZqFBGHvvdfslmjdGsySjb0uUSIhIiJ5bcmSNbsjUre5c8Pxxo2hffu1uyXa\ntAmDIqV2KZEQEZG8UJWZErvtFpKEVJfEnnvCLrvARvo0S4xeehER2aDSZ0qkWhv+97/6NVOirlAi\nISIitaaiIrQyjB8Pb78N//535pkSv/mNZkoUKiUSIiJSY1atgo8+gnHjQvIwfjwsXBi6HoqK4OCD\nNVOirlEiISIFraIifMsdOzbcpkwJH1D77htunTrBz36WdJR11w8/wAcfrE4a3n0Xvv8eNtkEDjgA\n+vQJycMBB4RZFFL3KJEQkYLiDlOnrk4c3nwzfOPdeGM46CA49tiwZsCAAbBoUXjMjjuuTir23Td8\nM27ePNHLKFhlZfDOOyFpGDcOJk4M3RQtWoTX/9prQ+JQVBRmU0jdp0RCRPKae1iRMJU4vPFG2GCo\nUaPwLfeCC+Cww8K/mzRZ83EzZoQPug8/DD9vuCF8W4Yw+j/earHPPtC0aTLXmM++/XZ10jB+fGj9\ncQ9rMXTpAsXF4eeee2qqZX2lREJE8s5XX62ZOHz+efiQKiqC004LicOBB667qdwMdt453E46KZSV\nl4eBfhMnrk4wRo6EFSugYcPQJZJqtdh337D6YX36Vu0Os2atmTh8+mk4tvPOoaWhT5+QOOy8sxZ1\nkkCJhIgkbt680EWRSh5SH14dOsDxx4fE4eCDQ/N5dTRsGGYE7LFHSEgAfvwxTD1MtVpMnAjDh4ek\no3HjsCpivFukfftwnrogNaMilTSMGxeSOAhJVLdu8Ne/htd+222TjVXylxIJEdngFi8OH1qpFoeP\nPw7l7dpB165w441wyCEbZpBk48bQsWO4nX12KPvhB/jXv1a3WowdC3/7W/jG3rRpqBvvFtlll8L4\ndr5yZZhRkRoYmT6jokePkDQceCBssUXS0UqhUCIhIrXu++/DGgJvvBE+lCdNCt+G27QJicOll8Kh\nh8J22yUdabDJJtC5c7ilLFkStlpOtVo89xzcfXc41qJFSCji3SLbb598chGfUTFuHLz3nmZUSM1T\nIiEiNW7FCnj//dVdFR98EL4Nt24duinOOSckDj//efIftlW1+eahleSQQ1aXzZ8fWixS3SLDh8Ot\nt4ZjW2+9OrFI/WzVqnZjTM2oSHVVaEaFbAhKJESk2latCh+mqRaHt9+G5ctD8/ihh4Zv7ocdBrvv\nXjiJQ1VstRUcdVS4pXz99ZqDOe+9N3QfQGiBibdaFBVVb9zHnDlrdlNoRoUkQYmEiGStoiKMa0i1\nOIwbB0uXwmabhQ+um24KiUOHDvXvA2zbbcNaFsceG+6npq/GB3PedBN89104vuuua7Za7LNP5m6G\n1IyK+MDIzz4LxzSjQpKkREJE1mt9i0BdeWVIHIqKtAtjOjNo2zbc/vjHUFZRsXoaairBeOaZ0CXU\noEGYVZJqtYDMMyoOPzysi6EZFZK0rP7Lm9lMYMcMh+5z9z5mNgw4Ne3YK+7+69g53gS6xI47MNjd\nz4vVaQkMAn4DVADPAH3d/fts4hWR3MUXgRo7ds1FoPr0CV0W6YtASdU0aBCmkbZvD6dGfzFXrgy7\nYMa7RR59NBzTjArJZ9l+d+gExGdQ7wW8Cjwd3XfgZeB0INWwtiLtHA48CFwTq7Msrc4TwNZAV6Ax\nMAwYDJycZbwiUkVffbV6jMPYsbktAiW5a9QIfvnLcDvrrFD2ww+rp5yK5KusEgl3XxC/b2bdgenu\nPj5WvMLd563nVMsqq2NmuwNHAkXu/lFU1gd4ycwucfc52cQsIpWbNg3uvx9Gjw7/hjUXgerSRXtS\nJGmTTZKOQGT9ch4GZWaNgJ7A0LRDh5jZt2Y21czuN7NMjXA9zWyemf3HzG42s/h/l87AolQSEXmd\n0JKxf67xikjgHloeuncPsyieeip0Uzz9NMydG0b+3313OK4kQkTWpzrDoo4HmgPDY2UvE8YzzAR2\nBm4BRplZZ3f3qM7jwOfA10AH4HZgN+D30fHWwNz4E7l7uZktjI6JSA5+/DEkDf37h1Ub99oLHnkk\nTBHUOAcRyVV1EolewMvxrgZ3fzp2fLKZ/QeYDhwCvBHVGZJW5xtgrJn93N1nViMeEclg4UIYPBgG\nDoRvvoGjj4Y77ggrSmqKoIhUV06JhJm1AboBx62rnrvPNLP5wC5EiUQGE6KfuxBaMuYAa6z/ZmYN\ngS2iY+vUr18/mqe1xxYXF1NcXLy+h4rUKZ99BvfcE1odysvhlFPgz38OO1yKSP1VUlJCSUnJGmVl\nZWU5n89W9zhk8SCz64GzgB3cvWId9bYndGMc6+4vVlLnQGAcsLe7fxINtpwMdIoNtjwCGAVsX9lg\nSzPrCEyaNGkSHTt2zPqaROoC97DmQP/+8MILYeXF88+H3r1rf3lmESlcpaWlFBUVQZjoUJrNY7Nu\nkTAzI0zvHBZPIsysGXAdYYzEHEILw23Ap8DoqE5boAchKVgA7A30B95y908A3H2qmY0GHjKz3oTp\nnwOBEs3YEMls5UoYMSIkEJMmhQWNHnoIevYMi0aJiNSWXLo2ugE7AI+klZcTBk+eCrQgDKYcDVzr\n7iujOj9Gj+8LNANmAyOAm9LO1YOwINXrhAWpRkaPEZGYRYtCwjBwIHz5ZVjt8JVX4IgjNP5BRDaM\nrBMJd3+NNRelSpUvB45a+xFr1PmSMPByfc+xGC0+JVKpGTNgwAB4+OHQGtGzJ/TrF2ZiiIhsSFoV\nX6RAuMO774bui2efDcskX3QRnHde2O1RRCQJSiRE8tyqVWFDp/79YcIEaNcOHnggzMLQyocikjQl\nEiJ5qqwMhgyBe++FL74I6z689BIcdVT925pbRPKXEgmRPDNrVkgehgyB5cvDypMXXQR77510ZCIi\na1MiIZIn3n8/dF8880zY46JPn7AGxLbbJh2ZiEjllEiIJGjVKnjuuZBAvPce7LorDBoEp56q7bpF\npDAokRBJwNKlMHRoWMJ61iw45JCwEuUxx2j8g4gUFiUSIhvQF1+ExaMefBCWLYMTT4SRIyGsTCsi\nUniUSIhsABMnhu6LESNgs83C3hcXXADbb590ZCIi1aNEQqSWlJeH7or+/eHtt2HnncNqlKefDptu\nmnR0IiI1Q4mESA377ruwdfeAAWEp64MPDitRdu8ODddaXF5EpLApkRCpIV9+GWZcDB4cBlP+4Q9Q\nUgL77Zd0ZCIitUeJhEg1lZaG7ounnoKmTeHss8MaEG3aJB2ZiEjtUyIhkoOKCnjxxZBAvPUW7LQT\n3Hkn9OoVBlOKiNQXmrEukqWXX4bdd4djj4UffwzTNz/7DPr2VRIhIvWPWiREsjB9Ovzxj7DvvjB8\nOHTunHREIiLJUiIhUkUrV0LPntCqFTz/vFofRERAiYRIld1wA3z4YVgTQkmEiEigMRIiVfD223DT\nTXDddXDAAUlHIyKSP5RIiKxHWRmcfHIYD3HllUlHIyKSX9S1IbIe558PixbBm2/CRvofIyKyBv1Z\nFFmHxx8Pt8ceC2tFiIjImtS1IVKJWbPgvPOgR48wW0NERNamREIkg1WrwriIli3hvvuSjkZEJH+p\na0Mkg1tvhffeC8tft2iRdDQiIvlLLRIiad5/H66/Hq66Cg46KOloRETymxIJkZilS8N4iE6d4Jpr\nko5GRCT/qWtDJObCC2HuXHj1VWjUKOloRETynxIJkcjTT8OwYfDII7DzzklHIyJSGNS1IQLMng3n\nnAN/+AOcdlrS0YiIFA4lElLvlZfDKafAppvCAw+AWdIRiYgUDnVtSL13550wbhyMGQNbbJF0NCIi\nhUUtElKvTZoEV18Nl10Ghx6adDQiIoVHiYTUW99/H5a/3ntv+Otfk45GRKQwZZVImNlMM6vIcBsY\nHR+W4diotHM0MbP7zGy+mS01s5Fm1iqtTksze9zMysxskZkNMbNm1b9ckdX69YMvvwybcjVunHQ0\nIiKFKdsWiU5A69jtcMCBp6PjDrwMbB2rU5x2jgHAMcAJQBdgW+CZtDpPAO2BrlHdLsDgLGMVqdSz\nz8JDD8GAAdCuXdLRiIgUrqwGW7r7gvh9M+sOTHf38bHiFe4+L9PjzWxzoBdwkru/FZWdAUwxs/3c\nfYKZtQeOBIrc/aOoTh/gJTO7xN3nZBOzSLqvv4Yzz4Tjjgs/RUQkdzmPkTCzRkBPYGjaoUPM7Fsz\nm2pm95tZfBx8ESF5GZMqcPdpwBdA56joAGBRKomIvE5o7dg/13hFACoqwjoRTZqEFglN9RQRqZ7q\nTP88HmgODI+VvUzoppgJ7AzcAowys87u7oSujh/dfUnaub6NjhH9nBs/6O7lZrYwVkckJwMGwOuv\nhyWwt9oq6WhERApfdRKJXsDL8a4Gd386dnyymf0HmA4cArxRjeeqsn79+tG8efM1yoqLiykuTh+q\nIfXNv/4FV14JF10Ehx+edDQiIskoKSmhpKRkjbKysrKcz5dTImFmbYBuwHHrqufuM81sPrALIZGY\nAzQ2s83TWiW2jo4R/UyfxdEQ2CJWp1J33303HTt2rOqlSD2xbFmY6rn77nDzzUlHIyKSnExfrktL\nSykqKsrpfLmOkehF6I4Yta5KZrY9sCXwTVQ0CVhFmI2RqtMOaAO8FxW9B7Qws31ip+oKGPBBjvFK\nPXfZZTBzJpSUhPERIiJSM7JukTAzA04Hhrl7Ray8GXAdYYzEHEIrxG3Ap8BoAHdfYmZDgf5mtghY\nCtwLvOPuE6I6U81sNPCQmfUGGgMDgRLN2JBcvPgi3HdfuO2xR9LRiIjULbl0bXQDdgAeSSsvBzoA\npwItgK8JCcS17r4yVq9fVHck0AR4BTg/7Vw9gEGE2RoVUd2+OcQq9dycOdCrFxxzDPTunXQ0IiJ1\nT9aJhLu/BjTMUL4cOKoKj18B9IluldVZDJycbWwice5wxhlhiufDD2uqp4hIbdDun1JnDRoEr7wC\no0ZBq1brry8iItnTpl1SJ33yCVx6KfTpA0cfnXQ0IiJ1lxIJqXOWLw9TPXfZBW67LeloRETqNnVt\nSJ1z5ZUwbRpMnAibbJJ0NCIidZsSCalTRo8Oy2DffTd06JB0NCIidZ+6NqTOmDcPTj8djjwSLrww\n6WhEROoQ+eAMAAAgAElEQVQHJRJSJ7jDn/4Eq1bBI49AA/1mi4hsEOrakDph8GD45z/h+edhm22S\njkZEpP7Q9zYpeFOmhB09zz0XfvvbpKMREalflEhIQVuxIkz1bNMG7ror6WhEROofdW1IQbvmGpg8\nGd5/H5o2TToaEZH6R4mEFKwxY+COO+D226Fjx6SjERGpn9S1IQVpwQI47TQ47DC4+OKkoxERqb+U\nSEjBcYdzzoFly+Dvf9dUTxGRJKlrQwrOww/DM8/AyJGw3XZJRyMiUr/pu5wUlE8/DatW/ulPcMIJ\nSUcjIiJKJKRgrFwJPXvCttuG/TRERCR56tqQgnH99fCvf8G778KmmyYdjYiIgBIJKRDjxsEtt8CN\nN8K++yYdjYiIpKhrQ/Le4sVw8slw0EFw+eVJRyMiInFKJCSvuYc9NJYsgcceg4YNk45IRETi1LUh\nee2xx+Cpp+DJJ8N+GiIikl/UIiF5a8YMOP98OPVUOPHEpKMREZFMlEhIXlq1KoyL2GorGDgw6WhE\nRKQy6tqQvHTjjTBhAowfD5tvnnQ0IiJSGbVISN5591244YawRXjnzklHIyIi66JEQvLKkiVh9cr9\n94errko6GhERWR91bUheueCCsEX4mDGwkX47RUTynv5US94oKYFHHw1bg7dtm3Q0IiJSFerakLzw\n+efQuzcUF4fZGiIiUhiUSEjiysvhlFOgeXO4/34wSzoiERGpKnVtSOJuvRXeeQfefBNatEg6GhER\nyYZaJCRREybAddfBlVfCwQcnHY2IiGQrq0TCzGaaWUWG21prD5rZA9GxC9PK30x7bLmZ3Z9Wp6WZ\nPW5mZWa2yMyGmFmz3C5R8tV330GPHtCxY0gmRESk8GTbtdEJiO+/uBfwKvB0vJKZHQ/sD3yV4RwO\nPAhcA6R6w5el1XkC2BroCjQGhgGDAQ3Dq0P69oU5c+Dll6FRo6SjERGRXGSVSLj7gvh9M+sOTHf3\n8bGy7YB7gCOBUZWcapm7z8t0wMx2jx5b5O4fRWV9gJfM7BJ3n5NNzJKfRo6Ehx+GoUNh112TjkZE\nRHKV8xgJM2sE9ASGxsoM+Dtwu7tPWcfDe5rZPDP7j5ndbGabxI51BhalkojI64SWjP1zjVfyx5df\nwtlnwwknwBlnJB2NiIhUR3VmbRwPNAeGx8quAH5090HreNzjwOfA10AH4HZgN+D30fHWwNz4A9y9\n3MwWRsekgFVUhG3BmzaFBx/UVE8RkUJXnUSiF/ByqqvBzIqAC4F91vUgdx8SuzvZzL4BxprZz919\nZjXiAaBfv340b958jbLi4mKKi4ure2qpAXfdFaZ5jhkDW2yRdDQiIvVPSUkJJSUla5SVlZXlfD5z\n9+wfZNYGmAEc5+4vRmV9gbsIXRApDYEK4At3z7josZk1Bb4DjnT318zsDOBOd98yVqchsBz4vbs/\nX8l5OgKTJk2aRMeOHbO+Jql9paVwwAHQrx/cdlvS0YiISEppaSlFRUUQxieWZvPYXFskegHfsuZg\nyr8Dr6XVezUqf2Qd59qHkHx8E91/D2hhZvvExkl0Jczw+CDHeCVh338fpnrutVfYIlxEROqGrBOJ\naEDl6cAwd69Ilbv7ImBRWt2VwBx3/yy63xboQUhAFgB7A/2Bt9z9k+g8U81sNPCQmfUmTP8cCJRo\nxkbhuvhi+OKL0CrRuHHS0YiISE3JpUWiG7AD625lSEnvN/kxenxfoBkwGxgB3JRWrwcwiDBbowIY\nGT1GCtBDD8HgwfDAA7D77klHIyIiNSnrRMLdX2PNRanWVbdt2v0vgUOq8LjFaPGpOuHee8PCU+ed\nF6Z8iohI3aK9NqTW3HprSCIuvhgGDdJUTxGRukiJhNQ4d7j22rAR17XXwh13KIkQEamrtI241Ch3\nuPTSsF7ErbfC5ZcnHZGIiNQmJRJSYyoq4IIL4G9/C2Mj+vRJOiIREaltSiSkRpSXw5lnwvDhYZbG\nmWcmHZGIiGwISiSk2lauhFNOCTt6Pvoo9OyZdEQiIrKhKJGQalmxAk48EUaNgqeeCjt6iohI/aFE\nQnK2bBn87ndhE67nnoNf/zrpiEREZENTIiE5WboUuneHiRPhpZega9ekIxIRkSQokZCsLV4MRx8N\nkyfDq6/CgQcmHZGIiCRFiYRkZf58OOIImDULxoyBffdNOiIREUmSEgmpsm++gcMPh3nzwriIDh2S\njkhERJKmREKqZPbsMA7i++/hrbe0i6eIiARKJGS9ZsyAww4L/x43DnbeOdl4REQkf2jTLlmnqVPh\n4IOhcWMYP15JhIiIrEmJhFTq44+hSxdo2TK0ROywQ9IRiYhIvlEiIRl9+CEccghsv30YWNm6ddIR\niYhIPlIiIWt5550wsLJdOxg7FrbaKumIREQkXymRkDWMHRvWidhnn7DYVIsWSUckIiL5TImE/GTU\nqLBfxkEHhX9vtlnSEYmISL5TIiEAPPMMHHccHHUUvPACNG2adEQiIlIIlEgIjz8etgL/3e9gxAho\n0iTpiEREpFAokajnhgyBU04Jt8cfh0aNko5IREQKiRKJeuzee+Gss+Dcc2HoUGjYMOmIRESk0CiR\nqKduuw369oWLL4b77oMG+k0QEZEc6OOjnnGH666DK66Aa6+FO+4As6SjEhGRQqVNu+oRd7j0Urjr\nLrjllpBMiIiIVIcSiXqiogL69IH774d77oELL0w6IhERqQuUSNQD5eVw5pkwfDg89FD4t4iISE1Q\nIlHHrVwZpnaOHAmPPgo9eyYdkYiI1CVKJOqwFSvCQlOjRsFTT8EJJyQdkYiI1DVKJOqoZcvCSpVv\nvgnPPRf20BAREalpSiTqoKVLoXt3mDgRXnopbAkuIiJSG7JaR8LMZppZRYbbwAx1H4iOXZhW3sTM\n7jOz+Wa21MxGmlmrtDotzexxMyszs0VmNsTMmuV2ifXL4sVhG/DSUhg9WkmEiIjUrmwXpOoEtI7d\nDgcceDpeycyOB/YHvspwjgHAMcAJQBdgW+CZtDpPAO2BrlHdLsDgLGOtd+bPh8MOg2nTYMyYsB24\niIhIbcqqa8PdF8Tvm1l3YLq7j4+VbQfcAxwJjEqrvznQCzjJ3d+Kys4AppjZfu4+wczaR48tcveP\nojp9gJfM7BJ3n5PtRdYHc+ZAt24wd24YF9GhQ9IRiYhIfZDzEtlm1gjoCQyNlRnwd+B2d5+S4WFF\nhORlTKrA3acBXwCdo6IDgEWpJCLyOqHlY/9c463LZs+GLl1g0SIYN05JhIiIbDjVGWx5PNAcGB4r\nuwL40d0HVfKY1tHxJWnl30bHUnXmxg+6e7mZLYzVkciMGaE7A0ISsfPOycYjIiL1S3USiV7Ay6mu\nBjMrAi4E9qmJwGT9pk4NgymbNoWxY2GHHZKOSERE6pucEgkzawN0A46LFR8E/AyYbau3k2wI9Dez\nP7t7W2AO0NjMNk9rldg6Okb0M30WR0Ngi1idSvXr14/mzZuvUVZcXExxcXEVr64wfPwxHH44bLUV\nvP46bLNN0hGJiEghKCkpoaSkZI2ysrKynM9n7p79g8yuB84CdnD3iqisJZD+cfYqYczEI+7+WTTY\nch5hsOWz0ePaAVOAA6LBlrsDk4FOscGWRxAGbm5f2WBLM+sITJo0aRIdO3bM+poKyYcfhimeO+0E\nr74akgkREZFclZaWUlRUBGGiQ2k2j826RSIaUHk6MCyVRAC4+yJgUVrdlcAcd/8sqrPEzIYSWikW\nAUuBe4F33H1CVGeqmY0GHjKz3kBjYCBQohkb8M47YZXKPfaAl1+GFi2SjkhEROqzXLo2ugE7AI9U\noW6m5o5+QDkwEmgCvAKcn1anBzCIMFujIqrbN4dY65SxY8OKlfvuC//8J2y2WdIRiYhIfZd1IuHu\nrxHGPlSlbtsMZSuAPtGtssctBk7ONra6bNSosHfGr34Fzz4bBliKiIgkLed1JGTD+cc/4Ljj4Kij\n4IUXlESIiEj+UCKR5554Av74x9AaMWIENGmSdEQiIiKrKZHIY0OGwMknwymnwOOPQ6NGSUckIiKy\nJiUSeWrQIDjrLDj3XBg6FBpWaVSKiIjIhqVEIg+9/z5ceCH8+c9w333QQO+SiIjkKX1E5ZmVK+Hs\ns6FjR7jzTli9SKiIiEj+qc5eG1IL7r4bJk+GiRPVnSEiIvlPLRJ5ZOZMuP566Ns3tEiIiIjkOyUS\necIdevcO+2b89a9JRyMiIlI16trIE089BaNHhwWnNt006WhERESqRi0SeWDRotCdccIJYS8NERGR\nQqFEIg9ccQX88APcc0/SkYiIiGRHXRsJe/ttePDBsADVdtslHY2IiEh21CKRoB9/hHPOgf33DytY\nioiIFBq1SCTojjtg2jQoLdWaESIiUpjUIpGQzz6DG26Aiy+GDh2SjkZERCQ3SiQSkFozYptt4Npr\nk45GREQkd+raSMBjj8GYMTBqFDRrlnQ0IiIiuVOLxAa2YAFcdBGceCIcfXTS0YiIiFSPEokN7NJL\nYdUqGDAg6UhERESqT10bG9Cbb8Ijj8DgwdC6ddLRiIiIVJ9aJDaQFSvCWhEHHghnnpl0NCIiIjVD\nLRIbyC23wPTp8Mwz0EDpm4iI1BH6SNsApk4NicRll8EvfpF0NCIiIjVHiUQtcw9dGjvsAFdfnXQ0\nIiIiNUtdG7Vs2DB46y147TXYZJOkoxEREalZapGoRfPmwSWXwMknQ7duSUcjIiJS85RI1KKLLw4/\n+/dPNg4REZHaoq6NWvL66/DoozB0KPzsZ0lHIyIiUjvUIlELfvghDLDs0gXOOCPpaERERGqPWiRq\nwU03wezZ8OKLYJZ0NCIiIrVHLRI1bPJkuO02uPJK2H33pKMRERGpXUokalBFBZxzDrRtGxIJERGR\nui6rRMLMZppZRYbbwOj4dWY2xcy+M7OFZvaame2Xdo430x5bbmb3p9VpaWaPm1mZmS0ysyFm1qz6\nl1u7hgyBd94Jm3I1aZJ0NCIiIrUv2xaJTkDr2O1wwIGno+PTgPOBPYEDgVnAq2a2ZewcDjwIbB2d\nYxvgsrTneQJoD3QFjgG6AIOzjHWDmjMHLr88DK485JCkoxEREdkwshps6e4L4vfNrDsw3d3HR8ef\nTDt+EfAnoAPwRuzQMnefl+k5zGx34EigyN0/isr6AC+Z2SXuPiebmDeUfv1go43gjjuSjkRERGTD\nyXmMhJk1AnoCQ9dx/BxgMfDvtMM9zWyemf3HzG42s/ji0Z2BRakkIvI6oSVj/1zjrU2vvAJPPhkW\nntpyy/XXFxERqSuqM/3zeKA5MDxeaGbHAE8CTYGvgcPdfWGsyuPA59GxDsDtwG7A76PjrYG58XO6\ne7mZLYyO5ZVly+C886Br17AUtoiISH1SnUSiF/Byhq6GscDewFbAWcAIM9vP3ecDuPuQWN3JZvYN\nMNbMfu7uM6sRTyL+8hf4+msYPVprRoiISP2TUyJhZm2AbsBx6cfc/QdgRnSbYGafEsZJ3FbJ6SZE\nP3cBZgJzgFZpz9cQ2CI6tk79+vWjefPma5QVFxdTXFy8vodm7eOP4a67QjKx6641fnoREZEaV1JS\nQklJyRplZWVlOZ/P3D37B5ldT2ht2MHdK9ZT93/A3939r5UcPxAYB+zt7p9Egy0nA51igy2PAEYB\n21c22NLMOgKTJk2aRMeOHbO+pmyVl8OBB8LSpfDRR9C4ca0/pYiISK0oLS2lqKgIwkSH0mwem3WL\nhJkZcDowLJ5EmFlT4CrgBeAbQtfGBcC2wIioTlugByEpWEDoAukPvOXunwC4+1QzGw08ZGa9gcbA\nQKAkn2ZsDB4MH3wA48criRARkforl66NbsAOwCNp5eXA7sCphCRiATAROMjdp0R1fowe3xdoBswm\nJBk3pZ2rBzCIMFujAhgZPSYvfP11WLnyrLPgoIOSjkZERCQ5WScS7v4a0DBD+QrghPU89kvgkCo8\nx2Igb+dA9O0Lm2wS9tQQERGpz7T7Z5ZefBFGjoSSEmjZMuloREREkqVNu7Lw3Xdw/vlw5JFw4olJ\nRyMiIpI8tUhk4brrYN48ePNNrRkhIiICSiSqrLQUBgyAm2+Gn/886WhERETyg7o2qqC8HM4+G37x\nC7jooqSjERERyR9qkaiCQYNCi8S770KjRklHIyIikj/UIrEes2fD1VdD795wwAFJRyMiIpJflEis\nR58+sNlmYWyEiIiIrEldG+vw7LPw/PMwYgSk7QMmIiIiqEWiUkuWhNaI3/wGTljnep0iIiL1lxKJ\nSlx9NSxaFAZaas0IERGRzNS1kcHEiSGBuPNO2HHHpKMRERHJX2qRSLNqVVgz4pe/hAsvTDoaERGR\n/KYWiTT33AMffwwffAAb6dURERFZJ7VIxMyaBddeGwZZduqUdDQiIiL5T4lExD3s7LnFFnDDDUlH\nIyIiUhjUeB8ZORJGjYLnngsLUImIiMj6qUUCWLw4DKw87jg49tikoxERESkcSiSA//f/4PvvYeDA\npCMREREpLPW+a+O99+CBB8Jsje23TzoaERGRwlKvWyRWrgxrRnTqBOedl3Q0IiIihadet0jcdRdM\nmQIffggNGyYdjYiISOGpty0S06fDX/4Cf/5zWMVSREREslcvEwn30JXRqlVIJkRERCQ39bJro6QE\nXn0VXnwRmjVLOhoREZHCVe9aJBYuhH794A9/gGOOSToaERGRwlbvEonLL4fly8N0TxEREameetW1\nMX48DBkC998P22yTdDQiIiKFr960SKxYAeecAwccEH6KiIhI9dWbFok77oDPPoPSUmhQb9InERGR\n2lUvPlI//RRuvBEuuQT22ivpaEREROqOOp9IuMO558J228E11yQdjYiISN1S57s2Hn0U3ngDRo+G\npk2TjkZERKRuqdMtEvPnw0UXQY8ecMQRSUcjIiJS92SVSJjZTDOryHAbGB2/zsymmNl3ZrbQzF4z\ns/3SztHEzO4zs/lmttTMRppZq7Q6Lc3scTMrM7NFZjbEzLJeg/LSS6G8HPr3z/aRIiIiUhXZtkh0\nAlrHbocDDjwdHZ8GnA/sCRwIzAJeNbMtY+cYABwDnAB0AbYFnkl7nieA9kDXqG4XYHA2gb7xBgwb\nFmZrbL11No8UERGRqspqjIS7L4jfN7PuwHR3Hx8dfzLt+EXAn4AOwBtmtjnQCzjJ3d+K6pwBTDGz\n/dx9gpm1B44Eitz9o6hOH+AlM7vE3eesL87ly8MAy4MOgl69srlCERERyUbOYyTMrBHQExi6juPn\nAIuBf0fFRYTkZUyqnrtPA74AOkdFBwCLUklE5HVCy8f+VYntlltg5kx48EGtGSEiIlKbqjNr43ig\nOTA8XmhmxwBPAk2Br4HD3X1hdLg18KO7L0k717fRsVSdufGD7l5uZgtjdSo1c2ZIJK64Atq3z/KK\nREREJCvVSSR6AS9n6GoYC+wNbAWcBYyIui3mV+O5qh5Ur340btyc0lL47W9DWXFxMcXFxRvi6UVE\nRPJaSUkJJSUla5SVlZXlfL6cEgkzawN0A45LP+buPwAzotsEM/uUME7iNmAO0NjMNk9rldg6Okb0\nM30WR0Ngi1idSi1ZcjdjxnTksMOyviwREZE6L9OX69LSUoqKinI6X64jCHoRuiNGVfE5mkT/ngSs\nIszGAMDM2gFtgPeioveAFma2T+wcXQEDPljfkx1zDEoiRERENpCsWyTMzIDTgWHuXhErbwpcBbwA\nfEPo2riAML1zBIC7LzGzoUB/M1sELAXuBd5x9wlRnalmNhp4yMx6A42BgUBJVWZs9OuX7RWJiIhI\nrnLp2ugG7AA8klZeDuwOnEpIIhYAE4GD3H1KrF6/qO5IQkvFK4S1J+J6AIMIszUqorp9qxJcy5ZZ\nXImIiIhUS9aJhLu/BjTMUL6CsMjU+h6/AugT3Sqrsxg4OdvYREREZMPSKgsiIiKSMyUSIiIikjMl\nEiIiIpIzJRIiIiKSMyUSIiIikjMlEiIiIpIzJRIiIiKSMyUSIiIikjMlEiIiIpIzJRIiIiKSMyUS\nIiIikjMlEiIiIpIzJRIiIiKSMyUSIiIikjMlEiIiIpIzJRIiIiKSMyUSIiIikjMlEiIiIpIzJRIi\nIiKSMyUSIiIikjMlEiIiIpIzJRIiIiKSMyUSIiIikjMlEiIiIpIzJRIiIiKSMyUSIiIikjMlEiIi\nIpIzJRIiIiKSMyUSIiIikjMlEiIiIpIzJRIiIiKSMyUSIiIikjMlEnmspKQk6RBqTF26FtD15LO6\ndC2g68lndelaqiOrRMLMZppZRYbbQDPbyMxuM7OPzew7M/vKzIab2TZp53gz7bHlZnZ/Wp2WZva4\nmZWZ2SIzG2JmzWriggtJXfolrUvXArqefFaXrgV0PfmsLl1LdWTbItEJaB27HQ448DTQFPgl8Bdg\nH+B4oB3wfNo5HHgQ2Do6xzbAZWl1ngDaA12BY4AuwOAsYxUREZFatlE2ld19Qfy+mXUHprv7+Kjo\nyLTjFwAfmNn27v5l7NAyd5+X6TnMbPfoPEXu/lFU1gd4ycwucfc52cQsIiIitSfnMRJm1gjoCQxd\nR7UWhBaIxWnlPc1snpn9x8xuNrNNYsc6A4tSSUTk9eg8++car4iIiNS8rFok0hwPNAeGZzpoZk2A\nW4En3P272KHHgc+Br4EOwO3AbsDvo+Otgbnxc7l7uZktjI5VZmOAKVOmZH0h+aqsrIzS0tKkw6gR\ndelaQNeTz+rStYCuJ5/VpWuJfXZunO1jzd1zelIzewVY4e7HZji2EfAPwviHQ9MSifS6hwBjgZ3d\nfaaZXQmc6u7t0+p9C1zr7hnHSphZD0KSIiIiIrnp6e5PZPOAnFokzKwN0A04LsOxjYARwA7AYetK\nIiITop+7ADOBOUCrtHM2BLaIjlVmNKGrZRawfL0XISIiIikbAzsRPkuzkmvXRi/gW2BUvDCWRLQl\ntEQsqsK59iGMf/gmuv8e0MLM9omNk+gKGPBBZSeJBoJmlUWJiIjIT97N5UFZd22YmRFaDh5396ti\n5RsBzxCmgP6GNcc5LHT3lWbWFuhBSEAWAHsD/YEv3P2w2LlGEVolegONgYeBCe5+StZXKCIiIrUm\nl0TicOAVoJ27/y9WviMwI706obXhUHcfZ2bbA48BvwCaAbMJYyluineBmFkLYBDQHagARgJ93X1Z\ndpcnIiIitSnnwZYiIiIi2mtDREREcqZEQkRERHJWJxIJMzs/2lDsBzN738z2TTqmXJnZwWb2QrTp\nWYWZ/TbpmHJlZlea2QQzW2Jm35rZs2a2W9Jx5crMzjWzf0ebyZWZ2btmdlTScdUEM7si+n3rn3Qs\nuTCz6zJsJvjfpOOqDjPb1sweNbP5ZrYs+t3rmHRc2VrXZo9Jx5YLM2tgZjeY2YzoffmfmV2ddFy5\nMrNNzWyAmc2KrudtM+uUzTkKPpEwsxOBu4DrCFNJ/w2MNrOtEg0sd82AfwHnEQaqFrKDgYGEpc27\nAY2AV9OWRC8ks4HLgY5AEWEhtefNrP06H5XnosT7bML/nUL2Cas3A2wNHJRsOLmLBpy/A6wg7D3U\nHrgYqMqU+nyzrs0eC9EVwDmEv9G7EzadvCzaW6oQDSUssdAT2BN4DXg9fefudSn4wZZm9j7wgbv3\nje4b4Q/+ve5+e6LBVZOZVQDHufsLScdSE6Lkbi7Qxd3fTjqemmBmC4BL3P2RpGPJhZltCkwiTLW+\nBvjI3S9KNqrsmdl1wLHuXnDf2DMxs1uBzu7+q6RjqWlmNgD4tbsXZOukmf0TmOPuZ8XKRhI2ozw1\nuciyZ2YbA0uB7u7+Sqz8Q2CUu19blfMUdItEtHFYETAmVeYhM3qdsPmX5JfUJm4Lkw6kuqLmzZOA\npoRF1ArVfcA/3X1s0oHUgF2jLsHpZvaYme2QdEDV0B340MyejroFS83szKSDqq4qbvaY794FuprZ\nrgBmtjdwIGkLNBaIjYCGhJavuB/IokWvOpt25YOtCC/Ct2nl3wLtNnw4UpmopWgA8La7F2zftZnt\nSUgcUpn88e4+NdmochMlQr8kND0XuveB04FphD1+rgfGmdme7v59gnHlqi2hlegu4CZgP+BeM1vh\n7o8mGln1rHOzxwJxK7A5MNXMyglfyK9y9yeTDSt77v6dmb0HXGNmUwmfnT0IX8Q/q+p5Cj2RkMJx\nP7AHIXMvZFMJK7I2J+xY+3cz61JoyUS0ONwAoJu7r0w6nupy9/j+AJ+Y2QTCLsN/BAqx26kBYTXf\na6L7/46S2HOBQk4kegEvu/u69k3KdycSPmxPAv5LSMbvMbOvCzTJO5mwevRXwCqglLDdRFFVT1Do\nicR8oJwwwCpua9a9wZdsQGY2CPg1cLC7f7O++vnM3VexegXXj8xsP6Av4dtjISkCfgaURq1FEFr3\nukSDxpp4AQ+gcvcyM/uUsBlgIfoGmJJWNgX4XQKx1Ih1bfZYYG4HbnH3EdH9yWa2E3AlBZjkuftM\n4NBoEPzm7v6tmT3J2itVV6qgx0hE36QmEUacAj81oXclx81HpGZFScSxhGXSv0g6nlrQAGiSdBA5\neB3Yi/Btau/o9iFhCfu9CzmJgJ8Gke7C6s0AC807rN09247QylKoMm72WICaEr7AxlVQ+J+nP0RJ\nREvCTKHnqvrYQm+RgLDp1zAzm0TYkrwf4Y0elmRQuTKzZoQ/gKlviW2jwTwL3X12cpFlz8zuB4qB\n3wLfm1mq5ajM3Qtuq3czuxl4GfgC2IwwaOxXwBFJxpWLaNzAGmNVzP5/e3esylEYxnH8+16BQSlh\ndAeKCxCzO0CUQcmoGDAYDEaLQRZldQEMbAYpyi6LKINJeQzPf/oX5f2X4+j7GU+dek5neH/nvM9z\nTnkDniOi+0n4zyul7AKn5EI7BGwC78Bxk3X1YA+4LKWskWOS48ACsPjtWX9U5wFvFjiMiI+Gy+nV\nKbBeSnkAbslx8FXgoNGqKpVSpsj15h4YJd+43PGDNbT1QSIiTjpjhVvklsY1MB0RT81WVm0MOCOn\nG05bOjEAAAC+SURBVIJstoJsTppvqqhKS+Q1nHcdnwOOfr2a3g2Q92EQeAVugKl/MvEA7f5uyTC5\nr9sPPAEXwEREPDdaVaWIuCqlzJCNfRvkH5dX2tjQ1zEJjNDOfpVuy8A2OfE0ADwC+51jbdQH7JAB\n/IX8SeZ6RHS/dflS678jIUmSmtPqPR1JktQsg4QkSapmkJAkSdUMEpIkqZpBQpIkVTNISJKkagYJ\nSZJUzSAhSZKqGSQkSVI1g4QkSapmkJAkSdU+ASLXQKKwpf6cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaa8099dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nndl_helpers as helpers\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Static functions.\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''Sigmoid activation.'''\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    '''Derivative of the sigmoid activation.\n",
    "    Used in back prop to compute the output error and the layer errors.'''\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def quadratic_cost(examples, outputs):\n",
    "    '''Compute the quadratic cost for the outputs relative the \n",
    "    examples (x,y) pairs. Assumes that outputs is a list of column \n",
    "    vectors representing the output activations for the given examples.'''\n",
    "    n = len(examples)\n",
    "    error_sum = 0\n",
    "    for ((x,y), o) in zip(examples, outputs):\n",
    "        error_sum += math.pow(np.sum(y - o), 2)\n",
    "    return (1 / (2*n)) * error_sum\n",
    "\n",
    "def quadratic_cost_deriv(output_activations, y):\n",
    "    '''Derivative of the quadratic cost function. Used in back prop\n",
    "    to compute the output error. Assumes output activations and y are\n",
    "    equal-sized column vectors.'''\n",
    "    return (output_activations - y)\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes=[10,10]):\n",
    "        '''Initialize member variables and randomly initialize \n",
    "        bias and weight values.'''\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        # By convention, there are no weights and biases for input layer.\n",
    "        # Biases are a (layer size X 1) column vector.\n",
    "        self.biases = [np.random.randn(sz, 1) for sz in sizes[1:]]\n",
    "        \n",
    "        # Each layer has a (m x n) matrix of weights,\n",
    "        # where m is the size of the layer and n is the size of \n",
    "        # the subsequent layer. \n",
    "        # e.g. layer 1 has size 10, layer 2 has size 11, then layer 1's\n",
    "        # weights are a (10 x 11) matrix.\n",
    "        self.weights = [np.random.randn(m,n) \n",
    "                        for n,m in zip(sizes[:-1],sizes[1:])]\n",
    "        return\n",
    "    \n",
    "    def SGD(self, train, epochs=30, mini_batch_size=10, eta=3.0, test_data=None):\n",
    "        '''Stochastic gradient descent trains the NN in mini-batches.\n",
    "        train should be a list of tuples (x,y) representing the training\n",
    "        samples and their correct outputs.'''\n",
    "        \n",
    "        correct_list = []\n",
    "        cost_list = []\n",
    "        \n",
    "        # SGD broken into epochs.\n",
    "        # Each epoch shuffles the training data into mini-batches\n",
    "        # and iterates over them to incrementally update weights\n",
    "        # and biases using back propogation.\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data and split it into\n",
    "            # non-overlapping mini-batches.\n",
    "            random.shuffle(train)\n",
    "            mini_batches = [\n",
    "                train[k:k+mini_batch_size]\n",
    "                for k in range(0, len(train), mini_batch_size)]\n",
    "            \n",
    "            # Use each of the mini batches to update biases\n",
    "            # and weights using back propogation.\n",
    "            for mb in mini_batches:\n",
    "                self.update_by_mini_batch(mb, eta)\n",
    "                \n",
    "            # Evaluate on test data if given, print results.\n",
    "            if test_data:\n",
    "                correct, cost = self.evaluate(test_data)\n",
    "                correct_list.append(correct)\n",
    "                cost_list.append(cost)\n",
    "                print(\"SGD Epoch %03d: %05d/%05d, cost = %.4lf\" %\n",
    "                     (epoch, correct, len(test_data), cost))\n",
    "            else:\n",
    "                print(\"SGD Epoch %d complete\" % (epoch))\n",
    "        \n",
    "        return correct_list, cost_list\n",
    "    \n",
    "    def update_by_mini_batch(self, mini_batch, eta):\n",
    "        '''Use back propogation algorithm to compute the gradients\n",
    "        for each (x,y) pair in the mini_batch. Use those gradients\n",
    "        to update self.weights and self.biases at each iteration.'''\n",
    "        \n",
    "        # Every weight and bias has a gradient value.\n",
    "        # All are initialized as 0.\n",
    "        grad_b_sum = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w_sum = [np.zeros(w.shape) for w in self.biases]\n",
    "        \n",
    "        # Loop over the (x,y) pairs. Compute the bias and weight\n",
    "        # gradients using back prop.\n",
    "        for x,y in mini_batch:\n",
    "            # Back prop returns matrices of the gradient corresponding\n",
    "            # to each weight and bias.\n",
    "            delta_grad_b, delta_grad_w = self.back_prop(x,y)\n",
    "            \n",
    "            # Add the gradients to the running sums.\n",
    "            grad_b_sum = [gb + dgb for gb, dgb in zip(grad_b_sum, delta_grad_b)]\n",
    "            grad_w_sum = [gw + dgw for gw, dgw in zip(grad_w_sum, delta_grad_w)]\n",
    "            \n",
    "        # Update the weights and biases.\n",
    "        self.weights = [w - ((eta / len(mini_batch)) * gwsum) \n",
    "                        for w, gwsum in zip(self.weights, grad_w_sum)]\n",
    "        self.biases = [b - ((eta / len(mini_batch)) * gbsum)\n",
    "                       for b, gbsum in zip(self.biases, grad_b_sum)]\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def back_prop(self, x, y):\n",
    "        '''Execute back propogation for a single (x,y) (input, output)\n",
    "        training pair. Return a tuple of the gradients (grad_b, grad_w)\n",
    "        where grad_b and grad_w are layer-by-layer lists of numpy arrays.\n",
    "        \n",
    "        grad_w[l][j][k] is the gradient for the weight from neuron k in layer\n",
    "        l - 1 to neuron j in layer l.\n",
    "        \n",
    "        grad_b[l][j] is the gradient for the bias for neuron j in layer l.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Both gradients initialized as zeros.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Duplicate forward prop, tracking a and z to compute deltas.\n",
    "        a = x   # First activation is the input.\n",
    "        A = [a] # List of all activations.\n",
    "        Z = []  # Keep track of the inner activation terms.\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)\n",
    "            A.append(a)\n",
    "            \n",
    "        # Backward pass, compute the error first.\n",
    "        delta = quadratic_cost_deriv(A[-1], y) * sigmoid_deriv(Z[-1])\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, A[-2].transpose())\n",
    "        \n",
    "        # Python negative indexing used to loop backward through layers.\n",
    "        for l in range(2, self.num_layers):\n",
    "            sd = sigmoid_deriv(Z[-l])\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sd\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, A[-l - 1].transpose())\n",
    "        \n",
    "        return grad_b, grad_w\n",
    "        \n",
    "    def evaluate(self, validate):\n",
    "        '''Evaluate using the member variable biases, weights and the given\n",
    "        validation data. The validation data should be a list of tuples (x,y).\n",
    "        Return the number of correct classifications and the cost.'''\n",
    "        \n",
    "        # Compute outputs for each (x,y) pair using forward prop.\n",
    "        outputs = [self.forward_prop(x) for (x,y) in validate]\n",
    "        \n",
    "        num_matches = sum([int(np.argmax(o) == np.argmax(y))\n",
    "                          for (o, (x,y)) in zip(outputs, validate)])\n",
    "        \n",
    "        \n",
    "        cost = quadratic_cost(validate, outputs)\n",
    "        \n",
    "        return num_matches, cost\n",
    "    \n",
    "    def forward_prop(self, inputs):\n",
    "        '''Computes the network output given the input a.\n",
    "        Uses biases and weights to multiply the activations as\n",
    "        they pass through the network.'''\n",
    "        \n",
    "        # Inputs are treated as the first activation.\n",
    "        a = inputs\n",
    "        \n",
    "        # The activation at layer i is the sigmoid function\n",
    "        # evaluated on the current weights and biases and the\n",
    "        # activation from layer i - 1.\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        \n",
    "        # The output is the last activation.\n",
    "        outputs = a\n",
    "        return outputs\n",
    "\n",
    "# Retrieve 60000 training, 8000 testing, 2000 validation samples.\n",
    "train, test, validate = helpers.get_mnist_data()\n",
    "\n",
    "net = Network([784,30,10])\n",
    "num_matches_list, cost_list = net.SGD(train, epochs=10, mini_batch_size=10, eta=3.0, test_data=test)\n",
    "num_matches, cost = net.evaluate(validate)\n",
    "\n",
    "print('Validation: %d / %d, %.4lf, cost = %.4lf' %\n",
    "    (num_matches, len(validate), num_matches / len(validate), cost))\n",
    "\n",
    "plt.title('Cost over epochs.')\n",
    "plt.plot(cost_list)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Number of matches over epochs.')\n",
    "plt.plot(num_matches_list)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
