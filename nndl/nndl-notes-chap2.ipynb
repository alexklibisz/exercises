{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning Notes\n",
    "\n",
    "Notes and equations from [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/)\n",
    "\n",
    "# Chapter 2\n",
    "\n",
    "- Backpropagation purpose: compute the gradient of the cost function to use in gradient descent.\n",
    "- The partial derivatives $\\frac{\\partial C}{\\partial w} and \\frac{\\partial C}{\\partial b}$ tell us how quickly the cost changes w.r.t changes in the weights and biases.\n",
    "\n",
    "## Warm up: a fast matrix-based approach to computing the output from a neural network.\n",
    "\n",
    "### Notation\n",
    "\n",
    "- $w^l_{jk}$ denotes the weight for the connection from the $k$th neuron in the $(l - 1)$th layer to the $j$th neuron in the $l$th layer. In code, this is expressed as a list of matrices: `w[l][j][k]` is the weight from neuron `k` in layer `l-1` to neuron `j` in layer `l`.\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz16.png)\n",
    "\n",
    "- $b^l_j$ denotes the bias of the $j$th neuron in the $l$th layer. In code, this is expressed as a list of column vectors: `b[l][j]` is the bias from all neurons in layer `l-1` to neuron `j` in layer `l`.\n",
    "- $a^l_j$ denotes the activation of the $j$th neuron in the $l$th layer. In code this is expressed as a list of column vectors: `a[l][j]` is the activation for neuron `j` in layer `l`.\n",
    "\n",
    "- The value of $a^l_j$ is computed: $a^l_j = \\sigma ( \\sum_k w^l_{jk} \\cdot a^{l-1}_k + b^l_j)$. Notice we are using the activation from the layer $l - 1$ to compute the activation at layer $l$. \n",
    "- The same values can also be vectorized by expressing activations, weights, and biases as vectors: $a^l = \\sigma (w^l \\cdot a^l-1 + b^l)$.\n",
    "- $z^l$ is the weighted input to activation function in layer $l$, and it uses the activation of layer $l - 1$: $z^l = w^l \\cdot a^{l-1} + b^l$.\n",
    "\n",
    "\n",
    "## Necessary assumptions about the cost function\n",
    "\n",
    "- Using the quadractic cost function $C = \\frac{1}{2n} \\sum_x (y(x) - a^L(x))^2$, where $y$ is the true output and $a^L(x)$ is the output activation given the current weights and biases.\n",
    "- Cost function can be written as an average of the inidividual sample costs: $C = \\frac{1}{n} \\sum_x C_x$, $C_x = \\frac{1}{2} (y - a^L)^2$\n",
    "- Cost function can be written as a function of the outputs: $C = C(a^L)$, $C = \\frac{1}{2} (y - a^L)^2 = \\frac{1}{2} \\sum_j (y_j - a^L_j)^2$\n",
    "\n",
    "## Hadamard product\n",
    "\n",
    "- Elementwise multiplication of two vectors of the same dimensions.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \n",
    "  \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\n",
    "\\tag{28}\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [8]]\n",
      "[[3 4]\n",
      " [6 8]]\n"
     ]
    }
   ],
   "source": [
    "# Hadamard example - it seems the np.multiply implicitly does the \n",
    "# hadamard if the dimensions are correct.\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([[1],[2]])\n",
    "b = np.array([[3],[4]])\n",
    "c = np.multiply(a,b)\n",
    "print(c)\n",
    "\n",
    "# Is the hadamard the same as the dot product with one matrix transposed? No.\n",
    "a = np.array([[1],[2]])\n",
    "b = np.array([[3],[4]])\n",
    "c = np.dot(a,b.transpose())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four fundamental equations behind backpropagation\n",
    "\n",
    "### Preliminary: measure of error $\\delta^l_j$ \n",
    "\n",
    "- $\\delta^l_j$ is the error in the $j$th neuron in the $l$th layer. Backprop gives us a procedure to compute this error, which is used to compute the partial derivatives w.r.t. to weight and bias, which are used to compute the gradient.\n",
    "- $\\delta^l_j = \\frac{\\partial C}{\\partial z^l_j}$ is the error at layer $l$ neuron $j$ is the partial derivative of $C$ w.r.t. the weighted input to that neuron ($z^l_j$). \"a measure of the error in the neuron\".\n",
    "\n",
    "### 1. Error in the output layer $\\delta^L$\n",
    "\n",
    "Element-wise form:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j).\n",
    "\\tag{BP1}\\end{eqnarray}$\n",
    "\n",
    "Vectorized form:\n",
    "\n",
    "$\\begin{eqnarray} \n",
    "  \\delta^L = (a^L-y) \\odot \\sigma'(z^L).\n",
    "\\tag{30}\\end{eqnarray}$\n",
    "\n",
    "\n",
    "- $\\frac{\\partial C}{\\partial a^L_j}$ measures how fast the cost is changing as a function of the $j$th output activation. If $C$ doesn't heavily depend on a particular neuron, then the error $\\delta^L_j$ will be small.\n",
    "- $\\sigma'(z^L_j)$ measures how fast the activation function $\\sigma$ is changing at $z^L_j$. \n",
    "\n",
    "### 2. $\\delta^l$ in terms of the error in the next layer $\\delta^{l+1}$\n",
    "\n",
    "To find the error of any individual layer:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\n",
    "\\tag{BP2}\\end{eqnarray}$\n",
    "\n",
    "- This equation gets applied to each layer, starting at the penultimate layer and moving backwards through the network.\n",
    "- Use BP1 to get $\\delta^L$, then BP2 to get $\\delta^{L-1}$, $\\delta^{L-2}$, ...\n",
    "\n",
    "### 3. Rate of change of the cost with respect to any bias in the network\n",
    "\n",
    "Element-wise form:\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j.\n",
    "\\tag{BP3}\\end{eqnarray}$\n",
    "\n",
    "Vectorized form:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial b} = \\delta,\n",
    "\\tag{31}\\end{eqnarray}$\n",
    "\n",
    "### 4. Rate of change of the cost with respect to any weight in the network\n",
    "\n",
    "Element-wise form:\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\n",
    "\\tag{BP4}\\end{eqnarray}$\n",
    "\n",
    "- ROC of $C$ w.r.t. the weight from neuron $k$ in layer $l-1$ to neuron $j$ in layer $l$.\n",
    "- $a^{l-1}_k$ is the activation of neuron $k$ in the previous layer $l-1$.\n",
    "- $\\delta^l_j$ is the error at neuron $j$ in the current layer $l$.\n",
    "- \"Incoming activation times outgoing error\".\n",
    "- Because this is a product, having a very low activation neuron will yield little change even if the error is high. This is a potential pitfall - \"learning slowly\".\n",
    "\n",
    "Simplified form:\n",
    "\n",
    "$\\begin{eqnarray}  \\frac{\\partial\n",
    "    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out},\n",
    "\\tag{32}\\end{eqnarray}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proofs of the four fundamental equations\n",
    "\n",
    "TODO\n",
    "\n",
    "## Backpropagation algorithm summarized\n",
    "\n",
    "1. **Input**: The raw features for a single training sample expressed as the activation $a^1$.\n",
    "2. **Feedforward**: For $l = 2,3,...,L$, compute and store the weighted inputs: $z^l = w^l a^{l - 1} + b^l$: and the activations $a^l = \\sigma(z^l)$.\n",
    "3. **Output error**: Compute the output error $\\delta^L = (a^L-y) \\odot \\sigma'(z^L)$.\n",
    "4. **Backpropagate the error**: Going backwards $l = L - 1, L - 2, ..., 2$, compute each $\\delta^{l} = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^{l})$.\n",
    "5. **Compute, store gradients**: At each layer, store the gradients for weight: $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$ and for bias: $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$. These can be stored as lists of matrices of the same size as the bias and weight matrices.\n",
    "6. **Return**: return the stored gradient values $\\nabla C$, which will be used to update the weights and biases for the next iteration.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1) Suppose we modify a single neuron in a feedforward network so that the output is given by $f(\\sum_j w_jx_j + b)$ where $f$ is some function other than the sigmoid. How should we modify backprop for this case?\n",
    "\n",
    "You would need to be able to compute the derivative of that activation function and replace $\\sigma'$ with $f'$ in equations BP1 and BP2.\n",
    "\n",
    "2) Suppose we replace the usual non-linear $\\sigma$ function with $\\sigma z = z$ throughout the network. Rewrite the backpropagation algorith for this case.\n",
    "\n",
    "Not rewriting the entire algorithm, but I believe this would also be a matter of modifying all places that use the derivative of the new $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Backprop\n",
    "\n",
    "This is just a summary of how backpropagation is used to determine the $\\delta w$ and $\\delta b$ that are applied at each iteration of gradient descent.\n",
    "\n",
    "1. Input a set of training examples (`update_by_mini_batch()` function call).\n",
    "2. For each training example $x$:\n",
    "    - Set input activation $a^{x,1}$.\n",
    "    - `back_prop()` function call.\n",
    "    - Feedforward: for each $l = 2,3,...,L$ compute the weighted input to the next layer: $z^{x,l} = w^la^{x,l-1} + b^l$ and the activation at that layer: $a^{x,l} = \\sigma(z^{x,l})$.\n",
    "    - Output error: Compute the error at the output layer: $\\delta^{x,L} = \\nabla_aC_x \\odot \\sigma'(z^{x,L}) = \\delta^L = (a^L-y) \\odot \\sigma'(z^L) = (a^L-y) \\odot \\sigma'(z^L)$.\n",
    "    - Backpropagate the error: For each $l = L -1, L -2, ..., 2$ compute $\\delta^{x,l} = ((w^{l+1})^T\\delta^{x,l+1}) \\odot \\sigma'(z^{x,l})$. \n",
    "\n",
    "3. Gradient descent: use the computed errors to update the weight and bias values:\n",
    "    - For each $l = L, L - 1,...,2$ update weights and biases according to rules:\n",
    "    \n",
    "    $w^l \\rightarrow w^l - \\frac{\\eta}{m}\\sum_x \\delta^{x,l}(a^{x,l-1})^T$\n",
    "    \n",
    "    $b^l \\rightarrow b^l - \\frac{\\eta}{m}\\sum_x \\delta^{x,l}$\n",
    "    \n",
    "\n",
    "## Back-propagation code annotated with the equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop(self, x, y):\n",
    "        '''Execute back propogation for a single (x,y) (input, output)\n",
    "        training pair. Return a tuple of the gradients (grad_b, grad_w)\n",
    "        where grad_b and grad_w are layer-by-layer lists of numpy arrays.\n",
    "        \n",
    "        grad_w[l][j][k] is the gradient for the weight from neuron k in layer\n",
    "        l - 1 to neuron j in layer l.\n",
    "        \n",
    "        grad_b[l][j] is the gradient for the bias for neuron j in layer l.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Both gradients initialized as zeros.\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # Duplicate forward prop, tracking a and z to compute deltas.\n",
    "        a = x   # First activation is the input.\n",
    "        A = [a] # Store the activations.\n",
    "        Z = []  # Store the weighted inputs to activation.\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a) + b # Compute and store the weighted input to the activation.\n",
    "            Z.append(z)\n",
    "            a = sigmoid(z)       # Compute and store the sigmoid activation.\n",
    "            A.append(a)\n",
    "            \n",
    "        # Backward pass, compute the error first.\n",
    "        output_delta = quadratic_cost_deriv(A[-1], y) * sigmoid_deriv(Z[-1]) # Eq. BP1.\n",
    "        grad_b[-1] = output_delta # Eq. BP3. Bias gradient at the last layer is the output delta.\n",
    "        grad_w[-1] = np.dot(delta, A[-2].transpose()) # Eq. BP4.\n",
    "        \n",
    "        # Python negative indexing used to loop backward through layers.\n",
    "        for l in range(2, self.num_layers):\n",
    "            sd = sigmoid_deriv(Z[-l]) \n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sd # Eq. BP2.\n",
    "            grad_b[-l] = delta # Eq. BP3.\n",
    "            grad_w[-l] = np.dot(delta, A[-l - 1].transpose()) # Eq. BP4.\n",
    "        \n",
    "        return grad_b, grad_w"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
